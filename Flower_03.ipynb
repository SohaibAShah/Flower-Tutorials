{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a52b986",
   "metadata": {},
   "source": [
    "# Build a strategy from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff5edcd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu\n",
      "Flower 1.20.0 / PyTorch 2.7.1+cu126\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets.utils.logging import disable_progress_bar\n",
    "\n",
    "disable_progress_bar()\n",
    "\n",
    "import flwr\n",
    "from flwr.client import Client, ClientApp, NumPyClient\n",
    "from flwr.common import Context\n",
    "from flwr.server import ServerApp, ServerConfig, ServerAppComponents\n",
    "from flwr.server.strategy import Strategy\n",
    "from flwr.simulation import run_simulation\n",
    "from flwr_datasets import FederatedDataset\n",
    "\n",
    "DEVICE = torch.device(\"cpu\")  # Try \"cuda\" to train on GPU\n",
    "print(f\"Training on {DEVICE}\")\n",
    "print(f\"Flower {flwr.__version__} / PyTorch {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0d8900",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "970bc13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets(partition_id, num_partitions: int):\n",
    "    fds = FederatedDataset(dataset=\"cifar10\", partitioners={\"train\": num_partitions})\n",
    "    partition = fds.load_partition(partition_id)\n",
    "    # Divide data on each node: 80% train, 20% test\n",
    "    partition_train_test = partition.train_test_split(test_size=0.2, seed=42)\n",
    "    pytorch_transforms = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    "    )\n",
    "\n",
    "    def apply_transforms(batch):\n",
    "        # Instead of passing transforms to CIFAR10(..., transform=transform)\n",
    "        # we will use this function to dataset.with_transform(apply_transforms)\n",
    "        # The transforms object is exactly the same\n",
    "        batch[\"img\"] = [pytorch_transforms(img) for img in batch[\"img\"]]\n",
    "        return batch\n",
    "\n",
    "    partition_train_test = partition_train_test.with_transform(apply_transforms)\n",
    "    trainloader = DataLoader(partition_train_test[\"train\"], batch_size=32, shuffle=True)\n",
    "    valloader = DataLoader(partition_train_test[\"test\"], batch_size=32)\n",
    "    testset = fds.load_split(\"test\").with_transform(apply_transforms)\n",
    "    testloader = DataLoader(testset, batch_size=32)\n",
    "    return trainloader, valloader, testloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580a4ff6",
   "metadata": {},
   "source": [
    "## Model training/evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46e0be62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def get_parameters(net) -> List[np.ndarray]:\n",
    "    return [val.cpu().numpy() for _, val in net.state_dict().items()]\n",
    "\n",
    "\n",
    "def set_parameters(net, parameters: List[np.ndarray]):\n",
    "    params_dict = zip(net.state_dict().keys(), parameters)\n",
    "    state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
    "    net.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "\n",
    "def train(net, trainloader, epochs: int):\n",
    "    \"\"\"Train the network on the training set.\"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters())\n",
    "    net.train()\n",
    "    for epoch in range(epochs):\n",
    "        correct, total, epoch_loss = 0, 0, 0.0\n",
    "        for batch in trainloader:\n",
    "            images, labels = batch[\"img\"], batch[\"label\"]\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(images)\n",
    "            loss = criterion(net(images), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Metrics\n",
    "            epoch_loss += loss\n",
    "            total += labels.size(0)\n",
    "            correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n",
    "        epoch_loss /= len(trainloader.dataset)\n",
    "        epoch_acc = correct / total\n",
    "        print(f\"Epoch {epoch+1}: train loss {epoch_loss}, accuracy {epoch_acc}\")\n",
    "\n",
    "\n",
    "def test(net, testloader):\n",
    "    \"\"\"Evaluate the network on the entire test set.\"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    correct, total, loss = 0, 0, 0.0\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in testloader:\n",
    "            images, labels = batch[\"img\"], batch[\"label\"]\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = net(images)\n",
    "            loss += criterion(outputs, labels).item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    loss /= len(testloader.dataset)\n",
    "    accuracy = correct / total\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52d031a",
   "metadata": {},
   "source": [
    "## Flower Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68497c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowerClient(NumPyClient):\n",
    "    def __init__(self, partition_id, net, trainloader, valloader):\n",
    "        self.partition_id = partition_id\n",
    "        self.net = net\n",
    "        self.trainloader = trainloader\n",
    "        self.valloader = valloader\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        print(f\"[Client {self.partition_id}] get_parameters\")\n",
    "        return get_parameters(self.net)\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        print(f\"[Client {self.partition_id}] fit, config: {config}\")\n",
    "        set_parameters(self.net, parameters)\n",
    "        train(self.net, self.trainloader, epochs=1)\n",
    "        return get_parameters(self.net), len(self.trainloader), {}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        print(f\"[Client {self.partition_id}] evaluate, config: {config}\")\n",
    "        set_parameters(self.net, parameters)\n",
    "        loss, accuracy = test(self.net, self.valloader)\n",
    "        return float(loss), len(self.valloader), {\"accuracy\": float(accuracy)}\n",
    "\n",
    "\n",
    "def client_fn(context: Context) -> Client:\n",
    "    net = Net().to(DEVICE)\n",
    "    partition_id = context.node_config[\"partition-id\"]\n",
    "    num_partitions = context.node_config[\"num-partitions\"]\n",
    "    trainloader, valloader, _ = load_datasets(partition_id, num_partitions)\n",
    "    return FlowerClient(partition_id, net, trainloader, valloader).to_client()\n",
    "\n",
    "\n",
    "# Create the ClientApp\n",
    "client = ClientApp(client_fn=client_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548d22c0",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d01a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_PARTITIONS = 10\n",
    "\n",
    "# Create a list of ClientConfig objects, one for each client\n",
    "client_configs = [{\"partition_id\": i} for i in range(NUM_PARTITIONS)]\n",
    "\n",
    "# Specify the resources each of your clients need\n",
    "# By default, each client will be allocated 1x CPU and 0x GPUs\n",
    "backend_config = {\"client_resources\": {\"num_cpus\": 1, \"num_gpus\": 0.0},\n",
    "                  \"client_configs\": client_configs  # Pass the client configurations\n",
    "                 }\n",
    "print(DEVICE)\n",
    "\n",
    "# When running on GPU, assign an entire GPU for each client\n",
    "if DEVICE == \"cuda\":\n",
    "    backend_config = {\"client_resources\": {\"num_cpus\": 1, \"num_gpus\": 1.0},\n",
    "                      \"client_configs\": client_configs  # Pass the client configurations\n",
    "                     }\n",
    "    print(DEVICE)\n",
    "    # Refer to our Flower framework documentation for more details about Flower simulations\n",
    "    # and how to set up the `backend_config`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ec73886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the model and get the parameters\n",
    "params = get_parameters(Net())\n",
    "from flwr.server.strategy import FedAvg\n",
    "from flwr.common import ndarrays_to_parameters\n",
    "def server_fn(context: Context) -> ServerAppComponents:\n",
    "    # Create FedAvg strategy\n",
    "    strategy = FedAvg(\n",
    "        fraction_fit=0.3,\n",
    "        fraction_evaluate=0.3,\n",
    "        min_fit_clients=3,\n",
    "        min_evaluate_clients=3,\n",
    "        min_available_clients=NUM_PARTITIONS,\n",
    "        initial_parameters=ndarrays_to_parameters(\n",
    "            params\n",
    "        ),  # Pass initial model parameters\n",
    "    )\n",
    "\n",
    "    # Configure the server for 3 rounds of training\n",
    "    config = ServerConfig(num_rounds=3)\n",
    "    return ServerAppComponents(strategy=strategy, config=config)\n",
    "\n",
    "# Create ServerApp\n",
    "server = ServerApp(server_fn=server_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "936b74a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      Starting Flower ServerApp, config: num_rounds=3, no round_timeout\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [INIT]\n",
      "\u001b[92mINFO \u001b[0m:      Using initial global parameters provided by strategy\n",
      "\u001b[92mINFO \u001b[0m:      Starting evaluation of initial global parameters\n",
      "\u001b[92mINFO \u001b[0m:      Evaluation returned no results (`None`)\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 1]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 3 clients (out of 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m [Client 5] fit, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m free(): double free detected in tcache 2\n",
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m *** SIGABRT received at time=1755085790 on cpu 9 ***\n",
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m PC: @     0x74142829eb2c  (unknown)  pthread_kill\n",
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m     @     0x741428245330  897224192  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m     @     0x74142824527e         32  raise\n",
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m     @     0x7414282288ff        192  abort\n",
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m     @     0x7414282297b6        288  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m     @     0x7414282a8ff5         16  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m     @     0x7414282ab55f         80  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m     @     0x7414282addae         64  cfree\n",
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m     @     0x7410fc915e45         48  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m     @     0x7410fc929889         48  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m     @     0x7410fc753bcd         64  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m     @     0x7410fc754fc5         80  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m     @     0x7410fc715264        288  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m     @     0x7410fc7ae812        208  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m     @     0x7410fc73a71b         32  cuInit\n",
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m     @     0x7411e0836e6a       1072  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m     @     0x7411e083a260        304  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m     @     0x7414282a1ed3        112  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m     @     0x7411e0883fa9         48  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m     @     0x7411e084e1da        256  cudaGetDeviceCount\n",
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m     @     0x74140869ea62  (unknown)  c10::cuda::device_count()\n",
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m     @ ... and at least 1 more frames\n",
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m [2025-08-13 19:49:50,606 E 84151 84151] logging.cc:440: *** SIGABRT received at time=1755085790 on cpu 9 ***\n",
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m [2025-08-13 19:49:50,606 E 84151 84151] logging.cc:440: PC: @     0x74142829eb2c  (unknown)  pthread_kill\n",
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m [2025-08-13 19:49:50,606 E 84151 84151] logging.cc:440:     @     0x741428245330  897224192  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m [2025-08-13 19:49:50,606 E 84151 84151] logging.cc:440:     @     0x74142824527e         32  raise\n",
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m [2025-08-13 19:49:50,606 E 84151 84151] logging.cc:440:     @     0x7414282288ff        192  abort\n",
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m [2025-08-13 19:49:50,607 E 84151 84151] logging.cc:440:     @     0x7414282297b6        288  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m [2025-08-13 19:49:50,607 E 84151 84151] logging.cc:440:     @     0x7414282a8ff5         16  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m [2025-08-13 19:49:50,607 E 84151 84151] logging.cc:440:     @     0x7414282ab55f         80  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m [2025-08-13 19:49:50,607 E 84151 84151] logging.cc:440:     @     0x7414282addae         64  cfree\n",
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m [2025-08-13 19:49:50,610 E 84151 84151] logging.cc:440:     @     0x7410fc915e45         48  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m [2025-08-13 19:49:50,614 E 84151 84151] logging.cc:440:     @     0x7410fc929889         48  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m [2025-08-13 19:49:50,618 E 84151 84151] logging.cc:440:     @     0x7410fc753bcd         64  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m [2025-08-13 19:49:50,622 E 84151 84151] logging.cc:440:     @     0x7410fc754fc5         80  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m [2025-08-13 19:49:50,626 E 84151 84151] logging.cc:440:     @     0x7410fc715264        288  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m [2025-08-13 19:49:50,630 E 84151 84151] logging.cc:440:     @     0x7410fc7ae812        208  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m [2025-08-13 19:49:50,630 E 84151 84151] logging.cc:440:     @     0x7410fc73a71b         32  cuInit\n",
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m [2025-08-13 19:49:50,630 E 84151 84151] logging.cc:440:     @     0x7411e0836e6a       1072  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m [2025-08-13 19:49:50,630 E 84151 84151] logging.cc:440:     @     0x7411e083a260        304  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m [2025-08-13 19:49:50,630 E 84151 84151] logging.cc:440:     @     0x7414282a1ed3        112  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m [2025-08-13 19:49:50,630 E 84151 84151] logging.cc:440:     @     0x7411e0883fa9         48  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m [2025-08-13 19:49:50,630 E 84151 84151] logging.cc:440:     @     0x7411e084e1da        256  cudaGetDeviceCount\n",
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m [2025-08-13 19:49:50,630 E 84151 84151] logging.cc:440:     @     0x74140869ea62  (unknown)  c10::cuda::device_count()\n",
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m [2025-08-13 19:49:50,630 E 84151 84151] logging.cc:440:     @ ... and at least 1 more frames\n",
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m Fatal Python error: Aborted\n",
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m Stack (most recent call first):\n",
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/torch/autograd/graph.py\", line 824 in _engine_run_backward\n",
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 353 in backward\n",
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/torch/_tensor.py\", line 648 in backward\n",
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m   File \"/tmp/ipykernel_82939/3619953001.py\", line 44 in train\n",
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m   File \"/tmp/ipykernel_82939/3136847900.py\", line 15 in fit\n",
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/client/numpy_client.py\", line 227 in _fit\n",
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/client/client.py\", line 224 in maybe_call_fit\n",
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py\", line 128 in handle_legacy_message_from_msgtype\n",
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/client/client_app.py\", line 128 in ffn\n",
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/client/client_app.py\", line 144 in __call__\n",
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 58 in run\n",
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/util/tracing/tracing_helper.py\", line 467 in _resume_span\n",
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 691 in actor_method_executor\n",
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 879 in main_loop\n",
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/workers/default_worker.py\", line 289 in <module>\n",
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=84151)\u001b[0m Extension modules: psutil._psutil_linux, psutil._psutil_posix, msgpack._cmsgpack, google._upb._message, setproctitle, yaml._yaml, charset_normalizer.md, requests.packages.charset_normalizer.md, requests.packages.chardet.md, ray._raylet, numpy._core._multiarray_umath, numpy.linalg._umath_linalg, grpc._cython.cygrpc, _cffi_backend, pyarrow.lib, pyarrow._json, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, PIL._imaging, PIL._imagingft, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.lib, pyarrow._compute, pandas._libs.ops, pandas._libs.hashing, pandas._libs.arrays, pandas._libs.tslib, pandas._libs.sparse, pandas._libs.internals, pandas._libs.indexing, pandas._libs.index, pandas._libs.writers, pandas._libs.join, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.json, pandas._libs.parsers, pandas._libs.testing, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, kiwisolver._cext, PIL._imagingmath (total: 96)\n",
      "\u001b[36m(ClientAppActor pid=84150)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=84150)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=84149)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=84149)\u001b[0m \n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 1862bc1288b65e2d36c998b801000000\n",
      "\tpid: 84151\n",
      "\tnamespace: 4fe358c0-5938-4bc7-9c32-0a4c03d9cdd2\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 1862bc1288b65e2d36c998b801000000\n",
      "\tpid: 84151\n",
      "\tnamespace: 4fe358c0-5938-4bc7-9c32-0a4c03d9cdd2\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff1862bc1288b65e2d36c998b801000000 Worker ID: 67f97b51ac6656b79edbc4610caecfa37378bf42765e52ed3545a418 Node ID: f02362524687d1d6771dcf5cefab620e1379578fc7a7a7b49d22184e Worker IP address: 172.30.170.62 Worker port: 45743 Worker PID: 84151 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[36m(ClientAppActor pid=84149)\u001b[0m [Client 1] fit, config: {}\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: edcf4613ab8d86609d330c7e01000000\n",
      "\tpid: 84150\n",
      "\tnamespace: 4fe358c0-5938-4bc7-9c32-0a4c03d9cdd2\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: edcf4613ab8d86609d330c7e01000000\n",
      "\tpid: 84150\n",
      "\tnamespace: 4fe358c0-5938-4bc7-9c32-0a4c03d9cdd2\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffffedcf4613ab8d86609d330c7e01000000 Worker ID: db03ceb05997869273596c0d6a5515ba1a1e7861f2f78e58959f7e84 Node ID: f02362524687d1d6771dcf5cefab620e1379578fc7a7a7b49d22184e Worker IP address: 172.30.170.62 Worker port: 33597 Worker PID: 84150 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: ccee1d849c99b7aa6b9da04501000000\n",
      "\tpid: 84149\n",
      "\tnamespace: 4fe358c0-5938-4bc7-9c32-0a4c03d9cdd2\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: ccee1d849c99b7aa6b9da04501000000\n",
      "\tpid: 84149\n",
      "\tnamespace: 4fe358c0-5938-4bc7-9c32-0a4c03d9cdd2\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 0 results and 3 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 3 clients (out of 10)\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: ccee1d849c99b7aa6b9da04501000000\n",
      "\tpid: 84149\n",
      "\tnamespace: 4fe358c0-5938-4bc7-9c32-0a4c03d9cdd2\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: edcf4613ab8d86609d330c7e01000000\n",
      "\tpid: 84150\n",
      "\tnamespace: 4fe358c0-5938-4bc7-9c32-0a4c03d9cdd2\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 1862bc1288b65e2d36c998b801000000\n",
      "\tpid: 84151\n",
      "\tnamespace: 4fe358c0-5938-4bc7-9c32-0a4c03d9cdd2\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: ccee1d849c99b7aa6b9da04501000000\n",
      "\tpid: 84149\n",
      "\tnamespace: 4fe358c0-5938-4bc7-9c32-0a4c03d9cdd2\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: edcf4613ab8d86609d330c7e01000000\n",
      "\tpid: 84150\n",
      "\tnamespace: 4fe358c0-5938-4bc7-9c32-0a4c03d9cdd2\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 1862bc1288b65e2d36c998b801000000\n",
      "\tpid: 84151\n",
      "\tnamespace: 4fe358c0-5938-4bc7-9c32-0a4c03d9cdd2\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 0 results and 3 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 2]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 3 clients (out of 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffffccee1d849c99b7aa6b9da04501000000 Worker ID: 88d509e48852087211b60fce240c5c8c5e0e51b3ba7abd921ee2fcd6 Node ID: f02362524687d1d6771dcf5cefab620e1379578fc7a7a7b49d22184e Worker IP address: 172.30.170.62 Worker port: 46699 Worker PID: 84149 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 1862bc1288b65e2d36c998b801000000\n",
      "\tpid: 84151\n",
      "\tnamespace: 4fe358c0-5938-4bc7-9c32-0a4c03d9cdd2\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: ccee1d849c99b7aa6b9da04501000000\n",
      "\tpid: 84149\n",
      "\tnamespace: 4fe358c0-5938-4bc7-9c32-0a4c03d9cdd2\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: edcf4613ab8d86609d330c7e01000000\n",
      "\tpid: 84150\n",
      "\tnamespace: 4fe358c0-5938-4bc7-9c32-0a4c03d9cdd2\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 1862bc1288b65e2d36c998b801000000\n",
      "\tpid: 84151\n",
      "\tnamespace: 4fe358c0-5938-4bc7-9c32-0a4c03d9cdd2\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: ccee1d849c99b7aa6b9da04501000000\n",
      "\tpid: 84149\n",
      "\tnamespace: 4fe358c0-5938-4bc7-9c32-0a4c03d9cdd2\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: edcf4613ab8d86609d330c7e01000000\n",
      "\tpid: 84150\n",
      "\tnamespace: 4fe358c0-5938-4bc7-9c32-0a4c03d9cdd2\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 0 results and 3 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 3 clients (out of 10)\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 1862bc1288b65e2d36c998b801000000\n",
      "\tpid: 84151\n",
      "\tnamespace: 4fe358c0-5938-4bc7-9c32-0a4c03d9cdd2\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: edcf4613ab8d86609d330c7e01000000\n",
      "\tpid: 84150\n",
      "\tnamespace: 4fe358c0-5938-4bc7-9c32-0a4c03d9cdd2\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 1862bc1288b65e2d36c998b801000000\n",
      "\tpid: 84151\n",
      "\tnamespace: 4fe358c0-5938-4bc7-9c32-0a4c03d9cdd2\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: ccee1d849c99b7aa6b9da04501000000\n",
      "\tpid: 84149\n",
      "\tnamespace: 4fe358c0-5938-4bc7-9c32-0a4c03d9cdd2\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: edcf4613ab8d86609d330c7e01000000\n",
      "\tpid: 84150\n",
      "\tnamespace: 4fe358c0-5938-4bc7-9c32-0a4c03d9cdd2\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: ccee1d849c99b7aa6b9da04501000000\n",
      "\tpid: 84149\n",
      "\tnamespace: 4fe358c0-5938-4bc7-9c32-0a4c03d9cdd2\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 0 results and 3 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 3]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 3 clients (out of 10)\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: edcf4613ab8d86609d330c7e01000000\n",
      "\tpid: 84150\n",
      "\tnamespace: 4fe358c0-5938-4bc7-9c32-0a4c03d9cdd2\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: ccee1d849c99b7aa6b9da04501000000\n",
      "\tpid: 84149\n",
      "\tnamespace: 4fe358c0-5938-4bc7-9c32-0a4c03d9cdd2\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: edcf4613ab8d86609d330c7e01000000\n",
      "\tpid: 84150\n",
      "\tnamespace: 4fe358c0-5938-4bc7-9c32-0a4c03d9cdd2\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 1862bc1288b65e2d36c998b801000000\n",
      "\tpid: 84151\n",
      "\tnamespace: 4fe358c0-5938-4bc7-9c32-0a4c03d9cdd2\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: ccee1d849c99b7aa6b9da04501000000\n",
      "\tpid: 84149\n",
      "\tnamespace: 4fe358c0-5938-4bc7-9c32-0a4c03d9cdd2\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 1862bc1288b65e2d36c998b801000000\n",
      "\tpid: 84151\n",
      "\tnamespace: 4fe358c0-5938-4bc7-9c32-0a4c03d9cdd2\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 0 results and 3 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 3 clients (out of 10)\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: ccee1d849c99b7aa6b9da04501000000\n",
      "\tpid: 84149\n",
      "\tnamespace: 4fe358c0-5938-4bc7-9c32-0a4c03d9cdd2\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: edcf4613ab8d86609d330c7e01000000\n",
      "\tpid: 84150\n",
      "\tnamespace: 4fe358c0-5938-4bc7-9c32-0a4c03d9cdd2\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: ccee1d849c99b7aa6b9da04501000000\n",
      "\tpid: 84149\n",
      "\tnamespace: 4fe358c0-5938-4bc7-9c32-0a4c03d9cdd2\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 1862bc1288b65e2d36c998b801000000\n",
      "\tpid: 84151\n",
      "\tnamespace: 4fe358c0-5938-4bc7-9c32-0a4c03d9cdd2\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: edcf4613ab8d86609d330c7e01000000\n",
      "\tpid: 84150\n",
      "\tnamespace: 4fe358c0-5938-4bc7-9c32-0a4c03d9cdd2\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 1862bc1288b65e2d36c998b801000000\n",
      "\tpid: 84151\n",
      "\tnamespace: 4fe358c0-5938-4bc7-9c32-0a4c03d9cdd2\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 0 results and 3 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [SUMMARY]\n",
      "\u001b[92mINFO \u001b[0m:      Run finished 3 round(s) in 43.49s\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[36m(ClientAppActor pid=84149)\u001b[0m free(): double free detected in tcache 2\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=84149)\u001b[0m *** SIGABRT received at time=1755085792 on cpu 10 ***\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=84149)\u001b[0m PC: @     0x79acdd09eb2c  (unknown)  pthread_kill\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=84149)\u001b[0m     @     0x79acc469ea62  (unknown)  c10::cuda::device_count()\u001b[32m [repeated 30x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=84149)\u001b[0m     @     0x79acdd04527e         32  raise\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=84149)\u001b[0m     @     0x79acdd0288ff        192  abort\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=84149)\u001b[0m     @     0x79acdd0addae         64  cfree\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=84149)\u001b[0m     @     0x79a9a553a71b         32  cuInit\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=84149)\u001b[0m     @     0x79aaa024e1da        256  cudaGetDeviceCount\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=84149)\u001b[0m     @ ... and at least 1 more frames\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=84149)\u001b[0m [2025-08-13 19:49:53,035 E 84149 84149] logging.cc:440: *** SIGABRT received at time=1755085793 on cpu 10 ***\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=84149)\u001b[0m [2025-08-13 19:49:53,035 E 84149 84149] logging.cc:440: PC: @     0x79acdd09eb2c  (unknown)  pthread_kill\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=84149)\u001b[0m [2025-08-13 19:49:53,062 E 84149 84149] logging.cc:440:     @     0x79acc469ea62  (unknown)  c10::cuda::device_count()\u001b[32m [repeated 30x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=84149)\u001b[0m [2025-08-13 19:49:53,035 E 84149 84149] logging.cc:440:     @     0x79acdd04527e         32  raise\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=84149)\u001b[0m [2025-08-13 19:49:53,035 E 84149 84149] logging.cc:440:     @     0x79acdd0288ff        192  abort\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=84149)\u001b[0m [2025-08-13 19:49:53,036 E 84149 84149] logging.cc:440:     @     0x79acdd0addae         64  cfree\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=84149)\u001b[0m [2025-08-13 19:49:53,062 E 84149 84149] logging.cc:440:     @     0x79a9a553a71b         32  cuInit\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=84149)\u001b[0m [2025-08-13 19:49:53,062 E 84149 84149] logging.cc:440:     @     0x79aaa024e1da        256  cudaGetDeviceCount\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=84149)\u001b[0m [2025-08-13 19:49:53,062 E 84149 84149] logging.cc:440:     @ ... and at least 1 more frames\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=84149)\u001b[0m Fatal Python error: Aborted\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=84149)\u001b[0m Stack (most recent call first):\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=84149)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/torch/autograd/graph.py\", line 824 in _engine_run_backward\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=84149)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/torch/_tensor.py\", line 648 in backward\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=84149)\u001b[0m   File \"/tmp/ipykernel_82939/3619953001.py\", line 44 in train\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=84149)\u001b[0m   File \"/tmp/ipykernel_82939/3136847900.py\", line 15 in fit\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=84149)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/client/numpy_client.py\", line 227 in _fit\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=84149)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/client/client.py\", line 224 in maybe_call_fit\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=84149)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py\", line 128 in handle_legacy_message_from_msgtype\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=84149)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/client/client_app.py\", line 128 in ffn\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=84149)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/client/client_app.py\", line 144 in __call__\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=84149)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 58 in run\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=84149)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/util/tracing/tracing_helper.py\", line 467 in _resume_span\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=84149)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 691 in actor_method_executor\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=84149)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 879 in main_loop\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=84149)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/workers/default_worker.py\", line 289 in <module>\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=84149)\u001b[0m Extension modules: psutil._psutil_linux, psutil._psutil_posix, msgpack._cmsgpack, google._upb._message, setproctitle, yaml._yaml, charset_normalizer.md, requests.packages.charset_normalizer.md, requests.packages.chardet.md, ray._raylet, numpy._core._multiarray_umath, numpy.linalg._umath_linalg, grpc._cython.cygrpc, _cffi_backend, pyarrow.lib, pyarrow._json, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, PIL._imaging, PIL._imagingft, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.lib, pyarrow._compute, pandas._libs.ops, pandas._libs.hashing, pandas._libs.arrays, pandas._libs.tslib, pandas._libs.sparse, pandas._libs.internals, pandas._libs.indexing, pandas._libs.index, pandas._libs.writers, pandas._libs.join, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.json, pandas._libs.parsers, pandas._libs.testing, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, kiwisolver._cext, PIL._imagingmath (total: 96)\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Run simulation\n",
    "run_simulation(\n",
    "    server_app=server,\n",
    "    client_app=client,\n",
    "    num_supernodes=NUM_PARTITIONS,\n",
    "    backend_config=backend_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3a964f",
   "metadata": {},
   "source": [
    "### Build a Strategy from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0db07ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "from flwr.common import (\n",
    "    EvaluateIns,\n",
    "    EvaluateRes,\n",
    "    FitIns,\n",
    "    FitRes,\n",
    "    Parameters,\n",
    "    Scalar,\n",
    "    ndarrays_to_parameters,\n",
    "    parameters_to_ndarrays,\n",
    ")\n",
    "from flwr.server.client_manager import ClientManager\n",
    "from flwr.server.client_proxy import ClientProxy\n",
    "from flwr.server.strategy.aggregate import aggregate, weighted_loss_avg\n",
    "\n",
    "\n",
    "class FedCustom(Strategy):\n",
    "    def __init__(\n",
    "        self,\n",
    "        fraction_fit: float = 1.0,\n",
    "        fraction_evaluate: float = 1.0,\n",
    "        min_fit_clients: int = 2,\n",
    "        min_evaluate_clients: int = 2,\n",
    "        min_available_clients: int = 2,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.fraction_fit = fraction_fit\n",
    "        self.fraction_evaluate = fraction_evaluate\n",
    "        self.min_fit_clients = min_fit_clients\n",
    "        self.min_evaluate_clients = min_evaluate_clients\n",
    "        self.min_available_clients = min_available_clients\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return \"FedCustom\"\n",
    "\n",
    "    def initialize_parameters(\n",
    "        self, client_manager: ClientManager\n",
    "    ) -> Optional[Parameters]:\n",
    "        \"\"\"Initialize global model parameters.\"\"\"\n",
    "        net = Net()\n",
    "        ndarrays = get_parameters(net)\n",
    "        return ndarrays_to_parameters(ndarrays)\n",
    "\n",
    "    def configure_fit(\n",
    "        self, server_round: int, parameters: Parameters, client_manager: ClientManager\n",
    "    ) -> List[Tuple[ClientProxy, FitIns]]:\n",
    "        \"\"\"Configure the next round of training.\"\"\"\n",
    "\n",
    "        # Sample clients\n",
    "        sample_size, min_num_clients = self.num_fit_clients(\n",
    "            client_manager.num_available()\n",
    "        )\n",
    "        clients = client_manager.sample(\n",
    "            num_clients=sample_size, min_num_clients=min_num_clients\n",
    "        )\n",
    "\n",
    "        # Create custom configs\n",
    "        n_clients = len(clients)\n",
    "        half_clients = n_clients // 2\n",
    "        standard_config = {\"lr\": 0.001}\n",
    "        higher_lr_config = {\"lr\": 0.003}\n",
    "        fit_configurations = []\n",
    "        for idx, client in enumerate(clients):\n",
    "            if idx < half_clients:\n",
    "                fit_configurations.append((client, FitIns(parameters, standard_config)))\n",
    "            else:\n",
    "                fit_configurations.append(\n",
    "                    (client, FitIns(parameters, higher_lr_config))\n",
    "                )\n",
    "        return fit_configurations\n",
    "\n",
    "    def aggregate_fit(\n",
    "        self,\n",
    "        server_round: int,\n",
    "        results: List[Tuple[ClientProxy, FitRes]],\n",
    "        failures: List[Union[Tuple[ClientProxy, FitRes], BaseException]],\n",
    "    ) -> Tuple[Optional[Parameters], Dict[str, Scalar]]:\n",
    "        \"\"\"Aggregate fit results using weighted average.\"\"\"\n",
    "\n",
    "        weights_results = [\n",
    "            (parameters_to_ndarrays(fit_res.parameters), fit_res.num_examples)\n",
    "            for _, fit_res in results\n",
    "        ]\n",
    "        parameters_aggregated = ndarrays_to_parameters(aggregate(weights_results))\n",
    "        metrics_aggregated = {}\n",
    "        return parameters_aggregated, metrics_aggregated\n",
    "\n",
    "    def configure_evaluate(\n",
    "        self, server_round: int, parameters: Parameters, client_manager: ClientManager\n",
    "    ) -> List[Tuple[ClientProxy, EvaluateIns]]:\n",
    "        \"\"\"Configure the next round of evaluation.\"\"\"\n",
    "        if self.fraction_evaluate == 0.0:\n",
    "            return []\n",
    "        config = {}\n",
    "        evaluate_ins = EvaluateIns(parameters, config)\n",
    "\n",
    "        # Sample clients\n",
    "        sample_size, min_num_clients = self.num_evaluation_clients(\n",
    "            client_manager.num_available()\n",
    "        )\n",
    "        clients = client_manager.sample(\n",
    "            num_clients=sample_size, min_num_clients=min_num_clients\n",
    "        )\n",
    "\n",
    "        # Return client/config pairs\n",
    "        return [(client, evaluate_ins) for client in clients]\n",
    "\n",
    "    def aggregate_evaluate(\n",
    "        self,\n",
    "        server_round: int,\n",
    "        results: List[Tuple[ClientProxy, EvaluateRes]],\n",
    "        failures: List[Union[Tuple[ClientProxy, EvaluateRes], BaseException]],\n",
    "    ) -> Tuple[Optional[float], Dict[str, Scalar]]:\n",
    "        \"\"\"Aggregate evaluation losses using weighted average.\"\"\"\n",
    "\n",
    "        if not results:\n",
    "            return None, {}\n",
    "\n",
    "        loss_aggregated = weighted_loss_avg(\n",
    "            [\n",
    "                (evaluate_res.num_examples, evaluate_res.loss)\n",
    "                for _, evaluate_res in results\n",
    "            ]\n",
    "        )\n",
    "        metrics_aggregated = {}\n",
    "        return loss_aggregated, metrics_aggregated\n",
    "\n",
    "    def evaluate(\n",
    "        self, server_round: int, parameters: Parameters\n",
    "    ) -> Optional[Tuple[float, Dict[str, Scalar]]]:\n",
    "        \"\"\"Evaluate global model parameters using an evaluation function.\"\"\"\n",
    "\n",
    "        # Let's assume we won't perform the global model evaluation on the server side.\n",
    "        return None\n",
    "\n",
    "    def num_fit_clients(self, num_available_clients: int) -> Tuple[int, int]:\n",
    "        \"\"\"Return sample size and required number of clients.\"\"\"\n",
    "        num_clients = int(num_available_clients * self.fraction_fit)\n",
    "        return max(num_clients, self.min_fit_clients), self.min_available_clients\n",
    "\n",
    "    def num_evaluation_clients(self, num_available_clients: int) -> Tuple[int, int]:\n",
    "        \"\"\"Use a fraction of available clients for evaluation.\"\"\"\n",
    "        num_clients = int(num_available_clients * self.fraction_evaluate)\n",
    "        return max(num_clients, self.min_evaluate_clients), self.min_available_clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6769fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      Starting Flower ServerApp, config: num_rounds=3, no round_timeout\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [INIT]\n",
      "\u001b[92mINFO \u001b[0m:      Using initial global parameters provided by strategy\n",
      "\u001b[92mINFO \u001b[0m:      Starting evaluation of initial global parameters\n",
      "\u001b[92mINFO \u001b[0m:      Evaluation returned no results (`None`)\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 1]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 3 clients (out of 10)\n",
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m free(): double free detected in tcache 2\n",
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m *** SIGABRT received at time=1755085836 on cpu 14 ***\n",
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m PC: @     0x7c838229eb2c  (unknown)  pthread_kill\n",
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m     @     0x7c8382245330  (unknown)  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m     @     0x7c838224527e         32  raise\n",
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m     @     0x7c83822288ff        192  abort\n",
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m     @     0x7c83822297b6        288  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m     @     0x7c83822a8ff5         16  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m     @     0x7c83822ab55f         80  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m     @     0x7c83822addae         64  cfree\n",
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m     @     0x7c8045f15e45         48  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m     @     0x7c8045f29889         48  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m     @     0x7c8045d53bcd         64  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m     @     0x7c8045d54fc5         80  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m     @     0x7c8045d15264        288  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m     @     0x7c8045dae812        208  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m     @     0x7c8045d3a71b         32  cuInit\n",
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m     @     0x7c8140636e6a       1072  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m     @     0x7c814063a260        304  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m     @     0x7c83822a1ed3        112  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m     @     0x7c8140683fa9         48  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m     @     0x7c814064e1da        256  cudaGetDeviceCount\n",
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m     @     0x7c83480d6a62  (unknown)  c10::cuda::device_count()\n",
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m     @ ... and at least 1 more frames\n",
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m [2025-08-13 19:50:36,055 E 86280 86280] logging.cc:440: *** SIGABRT received at time=1755085836 on cpu 14 ***\n",
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m [2025-08-13 19:50:36,055 E 86280 86280] logging.cc:440: PC: @     0x7c838229eb2c  (unknown)  pthread_kill\n",
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m [2025-08-13 19:50:36,055 E 86280 86280] logging.cc:440:     @     0x7c8382245330  (unknown)  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m [2025-08-13 19:50:36,055 E 86280 86280] logging.cc:440:     @     0x7c838224527e         32  raise\n",
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m [2025-08-13 19:50:36,055 E 86280 86280] logging.cc:440:     @     0x7c83822288ff        192  abort\n",
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m [2025-08-13 19:50:36,055 E 86280 86280] logging.cc:440:     @     0x7c83822297b6        288  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m [2025-08-13 19:50:36,055 E 86280 86280] logging.cc:440:     @     0x7c83822a8ff5         16  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m [2025-08-13 19:50:36,055 E 86280 86280] logging.cc:440:     @     0x7c83822ab55f         80  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m [2025-08-13 19:50:36,055 E 86280 86280] logging.cc:440:     @     0x7c83822addae         64  cfree\n",
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m [2025-08-13 19:50:36,059 E 86280 86280] logging.cc:440:     @     0x7c8045f15e45         48  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m [2025-08-13 19:50:36,063 E 86280 86280] logging.cc:440:     @     0x7c8045f29889         48  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m [2025-08-13 19:50:36,067 E 86280 86280] logging.cc:440:     @     0x7c8045d53bcd         64  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m [2025-08-13 19:50:36,071 E 86280 86280] logging.cc:440:     @     0x7c8045d54fc5         80  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m [2025-08-13 19:50:36,076 E 86280 86280] logging.cc:440:     @     0x7c8045d15264        288  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m [2025-08-13 19:50:36,080 E 86280 86280] logging.cc:440:     @     0x7c8045dae812        208  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m [2025-08-13 19:50:36,080 E 86280 86280] logging.cc:440:     @     0x7c8045d3a71b         32  cuInit\n",
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m [2025-08-13 19:50:36,080 E 86280 86280] logging.cc:440:     @     0x7c8140636e6a       1072  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m [2025-08-13 19:50:36,080 E 86280 86280] logging.cc:440:     @     0x7c814063a260        304  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m [2025-08-13 19:50:36,080 E 86280 86280] logging.cc:440:     @     0x7c83822a1ed3        112  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m [2025-08-13 19:50:36,080 E 86280 86280] logging.cc:440:     @     0x7c8140683fa9         48  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m [2025-08-13 19:50:36,080 E 86280 86280] logging.cc:440:     @     0x7c814064e1da        256  cudaGetDeviceCount\n",
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m [2025-08-13 19:50:36,080 E 86280 86280] logging.cc:440:     @     0x7c83480d6a62  (unknown)  c10::cuda::device_count()\n",
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m [2025-08-13 19:50:36,080 E 86280 86280] logging.cc:440:     @ ... and at least 1 more frames\n",
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m Fatal Python error: Aborted\n",
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m Stack (most recent call first):\n",
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/torch/autograd/graph.py\", line 824 in _engine_run_backward\n",
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 353 in backward\n",
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/torch/_tensor.py\", line 648 in backward\n",
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m   File \"/tmp/ipykernel_82939/3619953001.py\", line 44 in train\n",
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m   File \"/tmp/ipykernel_82939/3136847900.py\", line 15 in fit\n",
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/client/numpy_client.py\", line 227 in _fit\n",
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/client/client.py\", line 224 in maybe_call_fit\n",
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py\", line 128 in handle_legacy_message_from_msgtype\n",
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/client/client_app.py\", line 128 in ffn\n",
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/client/client_app.py\", line 144 in __call__\n",
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 58 in run\n",
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/util/tracing/tracing_helper.py\", line 467 in _resume_span\n",
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 691 in actor_method_executor\n",
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 879 in main_loop\n",
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/workers/default_worker.py\", line 289 in <module>\n",
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m Extension modules: psutil._psutil_linux, psutil._psutil_posix, msgpack._cmsgpack, google._upb._message, setproctitle, yaml._yaml, charset_normalizer.md, requests.packages.charset_normalizer.md, requests.packages.chardet.md, ray._raylet, numpy._core._multiarray_umath, numpy.linalg._umath_linalg, grpc._cython.cygrpc, _cffi_backend, pyarrow.lib, pyarrow._json, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, PIL._imaging, PIL._imagingft, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.lib, pyarrow._compute, pandas._libs.ops, pandas._libs.hashing, pandas._libs.arrays, pandas._libs.tslib, pandas._libs.sparse, pandas._libs.internals, pandas._libs.indexing, pandas._libs.index, pandas._libs.writers, pandas._libs.join, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.json, pandas._libs.parsers, pandas._libs.testing, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, kiwisolver._cext, PIL._imagingmath (total: 96)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=86280)\u001b[0m [Client 8] fit, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=86277)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=86277)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=86278)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=86278)\u001b[0m \n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 1c5b6e87c1170d4b13e8dae901000000\n",
      "\tpid: 86280\n",
      "\tnamespace: 8de6765c-d80b-4af6-b87b-545c02fc6fc0\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 1c5b6e87c1170d4b13e8dae901000000\n",
      "\tpid: 86280\n",
      "\tnamespace: 8de6765c-d80b-4af6-b87b-545c02fc6fc0\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff1c5b6e87c1170d4b13e8dae901000000 Worker ID: acfba6303ccaf193ece8ef755f7f7dc4795397d042eafb0b887eca86 Node ID: 84d9ee6386e9e0c77abbae6f06b0bbbaa74d617ab9d390619c98ae80 Worker IP address: 172.30.170.62 Worker port: 36609 Worker PID: 86280 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[36m(ClientAppActor pid=86278)\u001b[0m [Client 7] fit, config: {}\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 95df4bba0d54fb31fcbce98901000000\n",
      "\tpid: 86277\n",
      "\tnamespace: 8de6765c-d80b-4af6-b87b-545c02fc6fc0\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 95df4bba0d54fb31fcbce98901000000\n",
      "\tpid: 86277\n",
      "\tnamespace: 8de6765c-d80b-4af6-b87b-545c02fc6fc0\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff95df4bba0d54fb31fcbce98901000000 Worker ID: 463cf56ac79b5cdabd192354281a13c08c2168fe6eb0a3613dad479b Node ID: 84d9ee6386e9e0c77abbae6f06b0bbbaa74d617ab9d390619c98ae80 Worker IP address: 172.30.170.62 Worker port: 37715 Worker PID: 86277 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 13e9320af6f205b265cc117a01000000\n",
      "\tpid: 86278\n",
      "\tnamespace: 8de6765c-d80b-4af6-b87b-545c02fc6fc0\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 13e9320af6f205b265cc117a01000000\n",
      "\tpid: 86278\n",
      "\tnamespace: 8de6765c-d80b-4af6-b87b-545c02fc6fc0\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 0 results and 3 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 3 clients (out of 10)\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 13e9320af6f205b265cc117a01000000\n",
      "\tpid: 86278\n",
      "\tnamespace: 8de6765c-d80b-4af6-b87b-545c02fc6fc0\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 1c5b6e87c1170d4b13e8dae901000000\n",
      "\tpid: 86280\n",
      "\tnamespace: 8de6765c-d80b-4af6-b87b-545c02fc6fc0\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 13e9320af6f205b265cc117a01000000\n",
      "\tpid: 86278\n",
      "\tnamespace: 8de6765c-d80b-4af6-b87b-545c02fc6fc0\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 95df4bba0d54fb31fcbce98901000000\n",
      "\tpid: 86277\n",
      "\tnamespace: 8de6765c-d80b-4af6-b87b-545c02fc6fc0\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 1c5b6e87c1170d4b13e8dae901000000\n",
      "\tpid: 86280\n",
      "\tnamespace: 8de6765c-d80b-4af6-b87b-545c02fc6fc0\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 95df4bba0d54fb31fcbce98901000000\n",
      "\tpid: 86277\n",
      "\tnamespace: 8de6765c-d80b-4af6-b87b-545c02fc6fc0\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 0 results and 3 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 2]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 3 clients (out of 10)\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 1c5b6e87c1170d4b13e8dae901000000\n",
      "\tpid: 86280\n",
      "\tnamespace: 8de6765c-d80b-4af6-b87b-545c02fc6fc0\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 13e9320af6f205b265cc117a01000000\n",
      "\tpid: 86278\n",
      "\tnamespace: 8de6765c-d80b-4af6-b87b-545c02fc6fc0\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 1c5b6e87c1170d4b13e8dae901000000\n",
      "\tpid: 86280\n",
      "\tnamespace: 8de6765c-d80b-4af6-b87b-545c02fc6fc0\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 95df4bba0d54fb31fcbce98901000000\n",
      "\tpid: 86277\n",
      "\tnamespace: 8de6765c-d80b-4af6-b87b-545c02fc6fc0\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 13e9320af6f205b265cc117a01000000\n",
      "\tpid: 86278\n",
      "\tnamespace: 8de6765c-d80b-4af6-b87b-545c02fc6fc0\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 95df4bba0d54fb31fcbce98901000000\n",
      "\tpid: 86277\n",
      "\tnamespace: 8de6765c-d80b-4af6-b87b-545c02fc6fc0\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff13e9320af6f205b265cc117a01000000 Worker ID: 775ae76f0ed390e81816fc7086aefce1ecb4b4093e95fd912047c631 Node ID: 84d9ee6386e9e0c77abbae6f06b0bbbaa74d617ab9d390619c98ae80 Worker IP address: 172.30.170.62 Worker port: 35353 Worker PID: 86278 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 0 results and 3 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 3 clients (out of 10)\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 13e9320af6f205b265cc117a01000000\n",
      "\tpid: 86278\n",
      "\tnamespace: 8de6765c-d80b-4af6-b87b-545c02fc6fc0\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 95df4bba0d54fb31fcbce98901000000\n",
      "\tpid: 86277\n",
      "\tnamespace: 8de6765c-d80b-4af6-b87b-545c02fc6fc0\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 13e9320af6f205b265cc117a01000000\n",
      "\tpid: 86278\n",
      "\tnamespace: 8de6765c-d80b-4af6-b87b-545c02fc6fc0\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 1c5b6e87c1170d4b13e8dae901000000\n",
      "\tpid: 86280\n",
      "\tnamespace: 8de6765c-d80b-4af6-b87b-545c02fc6fc0\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 95df4bba0d54fb31fcbce98901000000\n",
      "\tpid: 86277\n",
      "\tnamespace: 8de6765c-d80b-4af6-b87b-545c02fc6fc0\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 1c5b6e87c1170d4b13e8dae901000000\n",
      "\tpid: 86280\n",
      "\tnamespace: 8de6765c-d80b-4af6-b87b-545c02fc6fc0\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 0 results and 3 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 3]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 3 clients (out of 10)\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 95df4bba0d54fb31fcbce98901000000\n",
      "\tpid: 86277\n",
      "\tnamespace: 8de6765c-d80b-4af6-b87b-545c02fc6fc0\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 1c5b6e87c1170d4b13e8dae901000000\n",
      "\tpid: 86280\n",
      "\tnamespace: 8de6765c-d80b-4af6-b87b-545c02fc6fc0\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 13e9320af6f205b265cc117a01000000\n",
      "\tpid: 86278\n",
      "\tnamespace: 8de6765c-d80b-4af6-b87b-545c02fc6fc0\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 95df4bba0d54fb31fcbce98901000000\n",
      "\tpid: 86277\n",
      "\tnamespace: 8de6765c-d80b-4af6-b87b-545c02fc6fc0\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 1c5b6e87c1170d4b13e8dae901000000\n",
      "\tpid: 86280\n",
      "\tnamespace: 8de6765c-d80b-4af6-b87b-545c02fc6fc0\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 13e9320af6f205b265cc117a01000000\n",
      "\tpid: 86278\n",
      "\tnamespace: 8de6765c-d80b-4af6-b87b-545c02fc6fc0\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 0 results and 3 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 3 clients (out of 10)\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 1c5b6e87c1170d4b13e8dae901000000\n",
      "\tpid: 86280\n",
      "\tnamespace: 8de6765c-d80b-4af6-b87b-545c02fc6fc0\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 13e9320af6f205b265cc117a01000000\n",
      "\tpid: 86278\n",
      "\tnamespace: 8de6765c-d80b-4af6-b87b-545c02fc6fc0\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 95df4bba0d54fb31fcbce98901000000\n",
      "\tpid: 86277\n",
      "\tnamespace: 8de6765c-d80b-4af6-b87b-545c02fc6fc0\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 1c5b6e87c1170d4b13e8dae901000000\n",
      "\tpid: 86280\n",
      "\tnamespace: 8de6765c-d80b-4af6-b87b-545c02fc6fc0\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 13e9320af6f205b265cc117a01000000\n",
      "\tpid: 86278\n",
      "\tnamespace: 8de6765c-d80b-4af6-b87b-545c02fc6fc0\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 95df4bba0d54fb31fcbce98901000000\n",
      "\tpid: 86277\n",
      "\tnamespace: 8de6765c-d80b-4af6-b87b-545c02fc6fc0\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 0 results and 3 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [SUMMARY]\n",
      "\u001b[92mINFO \u001b[0m:      Run finished 3 round(s) in 71.17s\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[36m(ClientAppActor pid=86278)\u001b[0m free(): double free detected in tcache 2\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=86278)\u001b[0m *** SIGABRT received at time=1755085837 on cpu 10 ***\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=86278)\u001b[0m PC: @     0x7f9a1e09eb2c  (unknown)  pthread_kill\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=86278)\u001b[0m     @     0x7f9a1e045330  (unknown)  (unknown)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=86278)\u001b[0m     @     0x7f9a1e04527e         32  raise\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=86278)\u001b[0m     @     0x7f9a1e0288ff        192  abort\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=86278)\u001b[0m     @     0x7f9a0baa5a62  (unknown)  c10::cuda::device_count()\u001b[32m [repeated 28x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=86278)\u001b[0m     @     0x7f9a1e0addae         64  cfree\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=86278)\u001b[0m     @     0x7f96e173a71b         32  cuInit\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=86278)\u001b[0m     @     0x7f97ddc4e1da        256  cudaGetDeviceCount\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=86278)\u001b[0m     @ ... and at least 1 more frames\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=86278)\u001b[0m [2025-08-13 19:50:37,223 E 86278 86278] logging.cc:440: *** SIGABRT received at time=1755085837 on cpu 10 ***\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=86278)\u001b[0m [2025-08-13 19:50:37,223 E 86278 86278] logging.cc:440: PC: @     0x7f9a1e09eb2c  (unknown)  pthread_kill\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=86278)\u001b[0m [2025-08-13 19:50:37,223 E 86278 86278] logging.cc:440:     @     0x7f9a1e045330  (unknown)  (unknown)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=86278)\u001b[0m [2025-08-13 19:50:37,223 E 86278 86278] logging.cc:440:     @     0x7f9a1e04527e         32  raise\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=86278)\u001b[0m [2025-08-13 19:50:37,223 E 86278 86278] logging.cc:440:     @     0x7f9a1e0288ff        192  abort\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=86278)\u001b[0m [2025-08-13 19:50:37,250 E 86278 86278] logging.cc:440:     @     0x7f9a0baa5a62  (unknown)  c10::cuda::device_count()\u001b[32m [repeated 28x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=86278)\u001b[0m [2025-08-13 19:50:37,224 E 86278 86278] logging.cc:440:     @     0x7f9a1e0addae         64  cfree\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=86278)\u001b[0m [2025-08-13 19:50:37,250 E 86278 86278] logging.cc:440:     @     0x7f96e173a71b         32  cuInit\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=86278)\u001b[0m [2025-08-13 19:50:37,250 E 86278 86278] logging.cc:440:     @     0x7f97ddc4e1da        256  cudaGetDeviceCount\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=86278)\u001b[0m [2025-08-13 19:50:37,250 E 86278 86278] logging.cc:440:     @ ... and at least 1 more frames\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=86278)\u001b[0m Fatal Python error: Aborted\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=86278)\u001b[0m Stack (most recent call first):\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=86278)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/torch/autograd/graph.py\", line 824 in _engine_run_backward\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=86278)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/torch/_tensor.py\", line 648 in backward\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=86278)\u001b[0m   File \"/tmp/ipykernel_82939/3619953001.py\", line 44 in train\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=86278)\u001b[0m   File \"/tmp/ipykernel_82939/3136847900.py\", line 15 in fit\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=86278)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/client/numpy_client.py\", line 227 in _fit\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=86278)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/client/client.py\", line 224 in maybe_call_fit\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=86278)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py\", line 128 in handle_legacy_message_from_msgtype\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=86278)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/client/client_app.py\", line 128 in ffn\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=86278)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/client/client_app.py\", line 144 in __call__\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=86278)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 58 in run\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=86278)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/util/tracing/tracing_helper.py\", line 467 in _resume_span\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=86278)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 691 in actor_method_executor\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=86278)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 879 in main_loop\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=86278)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/workers/default_worker.py\", line 289 in <module>\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=86278)\u001b[0m Extension modules: psutil._psutil_linux, psutil._psutil_posix, msgpack._cmsgpack, google._upb._message, setproctitle, yaml._yaml, charset_normalizer.md, requests.packages.charset_normalizer.md, requests.packages.chardet.md, ray._raylet, numpy._core._multiarray_umath, numpy.linalg._umath_linalg, grpc._cython.cygrpc, _cffi_backend, pyarrow.lib, pyarrow._json, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, PIL._imaging, PIL._imagingft, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.lib, pyarrow._compute, pandas._libs.ops, pandas._libs.hashing, pandas._libs.arrays, pandas._libs.tslib, pandas._libs.sparse, pandas._libs.internals, pandas._libs.indexing, pandas._libs.index, pandas._libs.writers, pandas._libs.join, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.json, pandas._libs.parsers, pandas._libs.testing, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, kiwisolver._cext, PIL._imagingmath (total: 96)\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def server_fn(context: Context) -> ServerAppComponents:\n",
    "    # Configure the server for just 3 rounds of training\n",
    "    config = ServerConfig(num_rounds=3)\n",
    "    return ServerAppComponents(\n",
    "        config=config,\n",
    "        strategy=FedCustom(),  # <-- pass the new strategy here\n",
    "    )\n",
    "\n",
    "\n",
    "# Run simulation\n",
    "run_simulation(\n",
    "    server_app=server,\n",
    "    client_app=client,\n",
    "    num_supernodes=NUM_PARTITIONS,\n",
    "    backend_config=backend_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc05a34d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flwr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
