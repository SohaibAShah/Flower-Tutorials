{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37b5e125",
   "metadata": {},
   "source": [
    "# Use a federated learning strategy\n",
    "Welcome Back! to the Flower federated learning tutorial!\n",
    "\n",
    "In this notebook, weâ€™ll begin to customize the federated learning system we built in the introductory notebook again, using the Flower framework, Flower Datasets, and PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb536452",
   "metadata": {},
   "source": [
    "## Step 0: Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50214f1d",
   "metadata": {},
   "source": [
    "### Loading dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35ba97a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-08-12 15:26:54,257\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda\n",
      "Flower 1.20.0 / PyTorch 2.8.0+cu128\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import flwr\n",
    "from flwr.client import Client, ClientApp, NumPyClient\n",
    "from flwr.server import ServerApp, ServerConfig, ServerAppComponents\n",
    "from flwr.server.strategy import FedAvg, FedAdagrad\n",
    "from flwr.simulation import run_simulation\n",
    "from flwr_datasets import FederatedDataset\n",
    "from flwr.common import ndarrays_to_parameters, NDArrays, Scalar, Context\n",
    "\n",
    "DEVICE = torch.device(\"cuda\")  # Try \"cuda\" to train on GPU\n",
    "print(f\"Training on {DEVICE}\")\n",
    "print(f\"Flower {flwr.__version__} / PyTorch {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c910863c",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee53fc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_PARTITIONS = 10\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "def load_datasets(partition_id: int, num_partitions: int):\n",
    "    fds = FederatedDataset(dataset=\"cifar10\", partitioners={\"train\": num_partitions})\n",
    "    partition = fds.load_partition(partition_id)\n",
    "    # Divide data on each node: 80% train, 20% test\n",
    "    partition_train_test = partition.train_test_split(test_size=0.2, seed=42)\n",
    "    pytorch_transforms = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    "    )\n",
    "\n",
    "    def apply_transforms(batch):\n",
    "        # Instead of passing transforms to CIFAR10(..., transform=transform)\n",
    "        # we will use this function to dataset.with_transform(apply_transforms)\n",
    "        # The transforms object is exactly the same\n",
    "        batch[\"img\"] = [pytorch_transforms(img) for img in batch[\"img\"]]\n",
    "        return batch\n",
    "\n",
    "    partition_train_test = partition_train_test.with_transform(apply_transforms)\n",
    "    trainloader = DataLoader(\n",
    "        partition_train_test[\"train\"], batch_size=BATCH_SIZE, shuffle=True\n",
    "    )\n",
    "    valloader = DataLoader(partition_train_test[\"test\"], batch_size=BATCH_SIZE)\n",
    "    testset = fds.load_split(\"test\").with_transform(apply_transforms)\n",
    "    testloader = DataLoader(testset, batch_size=BATCH_SIZE)\n",
    "    return trainloader, valloader, testloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d149217",
   "metadata": {},
   "source": [
    "### Model training/evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a34165db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def get_parameters(net) -> List[np.ndarray]:\n",
    "    return [val.cpu().numpy() for _, val in net.state_dict().items()]\n",
    "\n",
    "\n",
    "def set_parameters(net, parameters: List[np.ndarray]):\n",
    "    params_dict = zip(net.state_dict().keys(), parameters)\n",
    "    state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
    "    net.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "\n",
    "def train(net, trainloader, epochs: int):\n",
    "    \"\"\"Train the network on the training set.\"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters())\n",
    "    net.train()\n",
    "    for epoch in range(epochs):\n",
    "        correct, total, epoch_loss = 0, 0, 0.0\n",
    "        for batch in trainloader:\n",
    "            images, labels = batch[\"img\"], batch[\"label\"]\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(images)\n",
    "            loss = criterion(net(images), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Metrics\n",
    "            epoch_loss += loss\n",
    "            total += labels.size(0)\n",
    "            correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n",
    "        epoch_loss /= len(trainloader.dataset)\n",
    "        epoch_acc = correct / total\n",
    "        print(f\"Epoch {epoch+1}: train loss {epoch_loss}, accuracy {epoch_acc}\")\n",
    "\n",
    "\n",
    "def test(net, testloader):\n",
    "    \"\"\"Evaluate the network on the entire test set.\"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    correct, total, loss = 0, 0, 0.0\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in testloader:\n",
    "            images, labels = batch[\"img\"], batch[\"label\"]\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = net(images)\n",
    "            loss += criterion(outputs, labels).item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    loss /= len(testloader.dataset)\n",
    "    accuracy = correct / total\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e712e86",
   "metadata": {},
   "source": [
    "### Define the Flower ClientApp\n",
    "\n",
    "The first step toward creating a ClientApp is to implement a subclasses of flwr.client.Client or flwr.client.NumPyClient. We use NumPyClient in this tutorial because it is easier to implement and requires us to write less boilerplate. To implement NumPyClient, we create a subclass that implements the three methods get_parameters, fit, and evaluate:\n",
    "\n",
    "get_parameters: Return the current local model parameters\n",
    "\n",
    "fit: Receive model parameters from the server, train the model on the local data, and return the updated model parameters to the server\n",
    "\n",
    "evaluate: Receive model parameters from the server, evaluate the model on the local data, and return the evaluation result to the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4da46039",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowerClient(NumPyClient):\n",
    "    def __init__(self, partition_id, net, trainloader, valloader):\n",
    "        self.partition_id = partition_id\n",
    "        self.net = net\n",
    "        self.trainloader = trainloader\n",
    "        self.valloader = valloader\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        print(f\"[Client {self.partition_id}] get_parameters\")\n",
    "        return get_parameters(self.net)\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        print(f\"[Client {self.partition_id}] fit, config: {config}\")\n",
    "        set_parameters(self.net, parameters)\n",
    "        train(self.net, self.trainloader, epochs=1)\n",
    "        return get_parameters(self.net), len(self.trainloader), {}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        print(f\"[Client {self.partition_id}] evaluate, config: {config}\")\n",
    "        set_parameters(self.net, parameters)\n",
    "        loss, accuracy = test(self.net, self.valloader)\n",
    "        return float(loss), len(self.valloader), {\"accuracy\": float(accuracy)}\n",
    "\n",
    "\n",
    "def client_fn(context: Context) -> Client:\n",
    "    net = Net().to(DEVICE)\n",
    "\n",
    "    # Read the node_config to fetch data partition associated to this node\n",
    "    partition_id = context.node_config[\"partition-id\"]\n",
    "    num_partitions = context.node_config[\"num-partitions\"]\n",
    "\n",
    "    trainloader, valloader, _ = load_datasets(partition_id, num_partitions)\n",
    "    return FlowerClient(partition_id, net, trainloader, valloader).to_client()\n",
    "\n",
    "\n",
    "# Create the ClientApp\n",
    "client = ClientApp(client_fn=client_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31227269",
   "metadata": {},
   "source": [
    "## Strategy customization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977e46f6",
   "metadata": {},
   "source": [
    "### Server-side parameter initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "797b975a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the model and get the parameters\n",
    "params = get_parameters(Net())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8abbc2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def server_fn(context: Context) -> ServerAppComponents:\n",
    "    # Create FedAvg strategy\n",
    "    strategy = FedAvg(\n",
    "        fraction_fit=0.3,\n",
    "        fraction_evaluate=0.3,\n",
    "        min_fit_clients=3,\n",
    "        min_evaluate_clients=3,\n",
    "        min_available_clients=NUM_PARTITIONS,\n",
    "        initial_parameters=ndarrays_to_parameters(\n",
    "            params\n",
    "        ),  # Pass initial model parameters\n",
    "    )\n",
    "\n",
    "    # Configure the server for 3 rounds of training\n",
    "    config = ServerConfig(num_rounds=3)\n",
    "    return ServerAppComponents(strategy=strategy, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a2983de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ServerApp\n",
    "server = ServerApp(server_fn=server_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edd3cbb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      Starting Flower ServerApp, config: num_rounds=3, no round_timeout\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [INIT]\n",
      "\u001b[92mINFO \u001b[0m:      Using initial global parameters provided by strategy\n",
      "\u001b[92mINFO \u001b[0m:      Starting evaluation of initial global parameters\n",
      "\u001b[92mINFO \u001b[0m:      Evaluation returned no results (`None`)\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 1]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 3 clients (out of 10)\n",
      "\u001b[36m(ClientAppActor pid=43536)\u001b[0m free(): double free detected in tcache 2\n",
      "\u001b[36m(ClientAppActor pid=43536)\u001b[0m *** SIGABRT received at time=1754983823 on cpu 10 ***\n",
      "\u001b[36m(ClientAppActor pid=43536)\u001b[0m PC: @     0x7b847fe9eb2c  (unknown)  pthread_kill\n",
      "\u001b[36m(ClientAppActor pid=43536)\u001b[0m     @     0x7b847fe45330  (unknown)  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=43536)\u001b[0m     @     0x7b847fe4527e         32  raise\n",
      "\u001b[36m(ClientAppActor pid=43536)\u001b[0m     @     0x7b847fe288ff        192  abort\n",
      "\u001b[36m(ClientAppActor pid=43536)\u001b[0m     @     0x7b847fe297b6        288  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=43536)\u001b[0m     @     0x7b847fea8ff5         16  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=43536)\u001b[0m     @     0x7b847feab55f         80  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=43536)\u001b[0m     @     0x7b847feaddae         64  cfree\n",
      "\u001b[36m(ClientAppActor pid=43536)\u001b[0m     @     0x7b811dd15e45         48  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=43536)\u001b[0m     @     0x7b811dd29889         48  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=43536)\u001b[0m     @     0x7b811db53bcd         64  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=43536)\u001b[0m     @     0x7b811db54fc5         80  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=43536)\u001b[0m     @     0x7b811db15264        288  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=43536)\u001b[0m     @     0x7b811dbae812        208  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=43536)\u001b[0m     @     0x7b811db3a71b         32  cuInit\n",
      "\u001b[36m(ClientAppActor pid=43536)\u001b[0m     @     0x7b824544d99a        256  cudaGetDeviceCount\n",
      "\u001b[36m(ClientAppActor pid=43536)\u001b[0m     @     0x7b84738ade07  (unknown)  c10::cuda::(anonymous namespace)::device_count_impl()\n",
      "\u001b[36m(ClientAppActor pid=43536)\u001b[0m [2025-08-12 15:30:23,991 E 43536 43536] logging.cc:440: *** SIGABRT received at time=1754983823 on cpu 10 ***\n",
      "\u001b[36m(ClientAppActor pid=43536)\u001b[0m [2025-08-12 15:30:23,991 E 43536 43536] logging.cc:440: PC: @     0x7b847fe9eb2c  (unknown)  pthread_kill\n",
      "\u001b[36m(ClientAppActor pid=43536)\u001b[0m [2025-08-12 15:30:23,991 E 43536 43536] logging.cc:440:     @     0x7b847fe45330  (unknown)  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=43536)\u001b[0m [2025-08-12 15:30:23,991 E 43536 43536] logging.cc:440:     @     0x7b847fe4527e         32  raise\n",
      "\u001b[36m(ClientAppActor pid=43536)\u001b[0m [2025-08-12 15:30:23,991 E 43536 43536] logging.cc:440:     @     0x7b847fe288ff        192  abort\n",
      "\u001b[36m(ClientAppActor pid=43536)\u001b[0m [2025-08-12 15:30:23,991 E 43536 43536] logging.cc:440:     @     0x7b847fe297b6        288  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=43536)\u001b[0m [2025-08-12 15:30:23,991 E 43536 43536] logging.cc:440:     @     0x7b847fea8ff5         16  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=43536)\u001b[0m [2025-08-12 15:30:23,991 E 43536 43536] logging.cc:440:     @     0x7b847feab55f         80  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=43536)\u001b[0m [2025-08-12 15:30:23,991 E 43536 43536] logging.cc:440:     @     0x7b847feaddae         64  cfree\n",
      "\u001b[36m(ClientAppActor pid=43536)\u001b[0m [2025-08-12 15:30:23,997 E 43536 43536] logging.cc:440:     @     0x7b811dd15e45         48  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=43536)\u001b[0m [2025-08-12 15:30:24,003 E 43536 43536] logging.cc:440:     @     0x7b811dd29889         48  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=43536)\u001b[0m [2025-08-12 15:30:24,009 E 43536 43536] logging.cc:440:     @     0x7b811db53bcd         64  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=43536)\u001b[0m [2025-08-12 15:30:24,015 E 43536 43536] logging.cc:440:     @     0x7b811db54fc5         80  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=43536)\u001b[0m [2025-08-12 15:30:24,020 E 43536 43536] logging.cc:440:     @     0x7b811db15264        288  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=43536)\u001b[0m [2025-08-12 15:30:24,026 E 43536 43536] logging.cc:440:     @     0x7b811dbae812        208  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=43536)\u001b[0m [2025-08-12 15:30:24,026 E 43536 43536] logging.cc:440:     @     0x7b811db3a71b         32  cuInit\n",
      "\u001b[36m(ClientAppActor pid=43536)\u001b[0m [2025-08-12 15:30:24,026 E 43536 43536] logging.cc:440:     @     0x7b8245434a76       1088  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=43536)\u001b[0m [2025-08-12 15:30:24,026 E 43536 43536] logging.cc:440:     @     0x7b8245439638        320  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=43536)\u001b[0m [2025-08-12 15:30:24,026 E 43536 43536] logging.cc:440:     @     0x7b847fea1ed3        112  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=43536)\u001b[0m [2025-08-12 15:30:24,026 E 43536 43536] logging.cc:440:     @     0x7b8245488ed9         48  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=43536)\u001b[0m [2025-08-12 15:30:24,026 E 43536 43536] logging.cc:440:     @     0x7b824544d99a        256  cudaGetDeviceCount\n",
      "\u001b[36m(ClientAppActor pid=43536)\u001b[0m [2025-08-12 15:30:24,026 E 43536 43536] logging.cc:440:     @     0x7b84738ade07  (unknown)  c10::cuda::(anonymous namespace)::device_count_impl()\n",
      "\u001b[36m(ClientAppActor pid=43536)\u001b[0m Fatal Python error: Aborted\n",
      "\u001b[36m(ClientAppActor pid=43536)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=43536)\u001b[0m Stack (most recent call first):\n",
      "\u001b[36m(ClientAppActor pid=43536)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 412 in _lazy_init\n",
      "\u001b[36m(ClientAppActor pid=43536)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1355 in convert\n",
      "\u001b[36m(ClientAppActor pid=43536)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 955 in _apply\n",
      "\u001b[36m(ClientAppActor pid=43536)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 928 in _apply\n",
      "\u001b[36m(ClientAppActor pid=43536)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1369 in to\n",
      "\u001b[36m(ClientAppActor pid=43536)\u001b[0m   File \"/tmp/ipykernel_39776/2937135518.py\", line 26 in client_fn\n",
      "\u001b[36m(ClientAppActor pid=43536)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py\", line 96 in handle_legacy_message_from_msgtype\n",
      "\u001b[36m(ClientAppActor pid=43536)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/client/client_app.py\", line 128 in ffn\n",
      "\u001b[36m(ClientAppActor pid=43536)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/client/client_app.py\", line 144 in __call__\n",
      "\u001b[36m(ClientAppActor pid=43536)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 58 in run\n",
      "\u001b[36m(ClientAppActor pid=43536)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/util/tracing/tracing_helper.py\", line 467 in _resume_span\n",
      "\u001b[36m(ClientAppActor pid=43536)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 691 in actor_method_executor\n",
      "\u001b[36m(ClientAppActor pid=43536)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 879 in main_loop\n",
      "\u001b[36m(ClientAppActor pid=43536)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/workers/default_worker.py\", line 289 in <module>\n",
      "\u001b[36m(ClientAppActor pid=43536)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=43536)\u001b[0m Extension modules: psutil._psutil_linux, psutil._psutil_posix, msgpack._cmsgpack, google._upb._message, setproctitle, yaml._yaml, charset_normalizer.md, requests.packages.charset_normalizer.md, requests.packages.chardet.md, ray._raylet, numpy._core._multiarray_umath, numpy.linalg._umath_linalg, grpc._cython.cygrpc, _cffi_backend, pyarrow.lib, pyarrow._json, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, PIL._imaging, PIL._imagingft, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.lib, pyarrow._compute, pandas._libs.ops, pandas._libs.hashing, pandas._libs.arrays, pandas._libs.tslib, pandas._libs.sparse, pandas._libs.internals, pandas._libs.indexing, pandas._libs.index, pandas._libs.writers, pandas._libs.join, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.json, pandas._libs.parsers, pandas._libs.testing, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, kiwisolver._cext (total: 95)\n",
      "\u001b[36m(ClientAppActor pid=43538)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=43538)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=43537)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=43537)\u001b[0m \n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 71920b4002a263f3be3747ac01000000\n",
      "\tpid: 43536\n",
      "\tnamespace: 75f927c0-d138-47bb-994c-99369043e518\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 71920b4002a263f3be3747ac01000000\n",
      "\tpid: 43536\n",
      "\tnamespace: 75f927c0-d138-47bb-994c-99369043e518\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff71920b4002a263f3be3747ac01000000 Worker ID: 5bdb96ee9991d1ee07fbfd1dbc31250036d75747b5d5236a507d1724 Node ID: f6bf7376cd0596f9f43c361f02d7d40dc466d7fae34e1cd842dcf471 Worker IP address: 172.30.170.62 Worker port: 35237 Worker PID: 43536 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 2ca271c4c3281e19cd8f9ee801000000\n",
      "\tpid: 43537\n",
      "\tnamespace: 75f927c0-d138-47bb-994c-99369043e518\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 2ca271c4c3281e19cd8f9ee801000000\n",
      "\tpid: 43537\n",
      "\tnamespace: 75f927c0-d138-47bb-994c-99369043e518\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 2fac4a35ce225ba3d1841ce501000000\n",
      "\tpid: 43538\n",
      "\tnamespace: 75f927c0-d138-47bb-994c-99369043e518\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 2fac4a35ce225ba3d1841ce501000000\n",
      "\tpid: 43538\n",
      "\tnamespace: 75f927c0-d138-47bb-994c-99369043e518\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 0 results and 3 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 3 clients (out of 10)\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 2fac4a35ce225ba3d1841ce501000000\n",
      "\tpid: 43538\n",
      "\tnamespace: 75f927c0-d138-47bb-994c-99369043e518\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 2fac4a35ce225ba3d1841ce501000000\n",
      "\tpid: 43538\n",
      "\tnamespace: 75f927c0-d138-47bb-994c-99369043e518\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 71920b4002a263f3be3747ac01000000\n",
      "\tpid: 43536\n",
      "\tnamespace: 75f927c0-d138-47bb-994c-99369043e518\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 2ca271c4c3281e19cd8f9ee801000000\n",
      "\tpid: 43537\n",
      "\tnamespace: 75f927c0-d138-47bb-994c-99369043e518\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 71920b4002a263f3be3747ac01000000\n",
      "\tpid: 43536\n",
      "\tnamespace: 75f927c0-d138-47bb-994c-99369043e518\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 2ca271c4c3281e19cd8f9ee801000000\n",
      "\tpid: 43537\n",
      "\tnamespace: 75f927c0-d138-47bb-994c-99369043e518\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 0 results and 3 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 2]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 3 clients (out of 10)\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 71920b4002a263f3be3747ac01000000\n",
      "\tpid: 43536\n",
      "\tnamespace: 75f927c0-d138-47bb-994c-99369043e518\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 2ca271c4c3281e19cd8f9ee801000000\n",
      "\tpid: 43537\n",
      "\tnamespace: 75f927c0-d138-47bb-994c-99369043e518\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff2fac4a35ce225ba3d1841ce501000000 Worker ID: a084e6da2cad19e8216af54a384a427ecf510df430202c412bb3ab70 Node ID: f6bf7376cd0596f9f43c361f02d7d40dc466d7fae34e1cd842dcf471 Worker IP address: 172.30.170.62 Worker port: 40021 Worker PID: 43538 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 2fac4a35ce225ba3d1841ce501000000\n",
      "\tpid: 43538\n",
      "\tnamespace: 75f927c0-d138-47bb-994c-99369043e518\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 71920b4002a263f3be3747ac01000000\n",
      "\tpid: 43536\n",
      "\tnamespace: 75f927c0-d138-47bb-994c-99369043e518\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 2ca271c4c3281e19cd8f9ee801000000\n",
      "\tpid: 43537\n",
      "\tnamespace: 75f927c0-d138-47bb-994c-99369043e518\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 2fac4a35ce225ba3d1841ce501000000\n",
      "\tpid: 43538\n",
      "\tnamespace: 75f927c0-d138-47bb-994c-99369043e518\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 0 results and 3 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 3 clients (out of 10)\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 71920b4002a263f3be3747ac01000000\n",
      "\tpid: 43536\n",
      "\tnamespace: 75f927c0-d138-47bb-994c-99369043e518\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 2ca271c4c3281e19cd8f9ee801000000\n",
      "\tpid: 43537\n",
      "\tnamespace: 75f927c0-d138-47bb-994c-99369043e518\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 2fac4a35ce225ba3d1841ce501000000\n",
      "\tpid: 43538\n",
      "\tnamespace: 75f927c0-d138-47bb-994c-99369043e518\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 71920b4002a263f3be3747ac01000000\n",
      "\tpid: 43536\n",
      "\tnamespace: 75f927c0-d138-47bb-994c-99369043e518\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 2ca271c4c3281e19cd8f9ee801000000\n",
      "\tpid: 43537\n",
      "\tnamespace: 75f927c0-d138-47bb-994c-99369043e518\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 2fac4a35ce225ba3d1841ce501000000\n",
      "\tpid: 43538\n",
      "\tnamespace: 75f927c0-d138-47bb-994c-99369043e518\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 0 results and 3 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 3]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 3 clients (out of 10)\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 2fac4a35ce225ba3d1841ce501000000\n",
      "\tpid: 43538\n",
      "\tnamespace: 75f927c0-d138-47bb-994c-99369043e518\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 2ca271c4c3281e19cd8f9ee801000000\n",
      "\tpid: 43537\n",
      "\tnamespace: 75f927c0-d138-47bb-994c-99369043e518\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 71920b4002a263f3be3747ac01000000\n",
      "\tpid: 43536\n",
      "\tnamespace: 75f927c0-d138-47bb-994c-99369043e518\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 2fac4a35ce225ba3d1841ce501000000\n",
      "\tpid: 43538\n",
      "\tnamespace: 75f927c0-d138-47bb-994c-99369043e518\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 2ca271c4c3281e19cd8f9ee801000000\n",
      "\tpid: 43537\n",
      "\tnamespace: 75f927c0-d138-47bb-994c-99369043e518\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 71920b4002a263f3be3747ac01000000\n",
      "\tpid: 43536\n",
      "\tnamespace: 75f927c0-d138-47bb-994c-99369043e518\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 0 results and 3 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 3 clients (out of 10)\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 71920b4002a263f3be3747ac01000000\n",
      "\tpid: 43536\n",
      "\tnamespace: 75f927c0-d138-47bb-994c-99369043e518\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 71920b4002a263f3be3747ac01000000\n",
      "\tpid: 43536\n",
      "\tnamespace: 75f927c0-d138-47bb-994c-99369043e518\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 2fac4a35ce225ba3d1841ce501000000\n",
      "\tpid: 43538\n",
      "\tnamespace: 75f927c0-d138-47bb-994c-99369043e518\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 2ca271c4c3281e19cd8f9ee801000000\n",
      "\tpid: 43537\n",
      "\tnamespace: 75f927c0-d138-47bb-994c-99369043e518\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 2fac4a35ce225ba3d1841ce501000000\n",
      "\tpid: 43538\n",
      "\tnamespace: 75f927c0-d138-47bb-994c-99369043e518\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 2ca271c4c3281e19cd8f9ee801000000\n",
      "\tpid: 43537\n",
      "\tnamespace: 75f927c0-d138-47bb-994c-99369043e518\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 0 results and 3 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [SUMMARY]\n",
      "\u001b[92mINFO \u001b[0m:      Run finished 3 round(s) in 29.92s\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[36m(ClientAppActor pid=43537)\u001b[0m free(): double free detected in tcache 2\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=43537)\u001b[0m *** SIGABRT received at time=1754983823 on cpu 3 ***\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=43537)\u001b[0m PC: @     0x74520ee9eb2c  (unknown)  pthread_kill\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=43537)\u001b[0m     @     0x74520ee45330  (unknown)  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=43537)\u001b[0m     @     0x74520ee4527e         32  raise\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=43537)\u001b[0m     @     0x74520ee288ff        192  abort\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=43537)\u001b[0m     @     0x744fc9488ed9         48  (unknown)\u001b[32m [repeated 31x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=43537)\u001b[0m     @     0x74520eeaddae         64  cfree\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=43537)\u001b[0m     @     0x744eb0b3a71b         32  cuInit\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=43537)\u001b[0m     @     0x744fc944d99a        256  cudaGetDeviceCount\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=43537)\u001b[0m     @     0x7452028ade07  (unknown)  c10::cuda::(anonymous namespace)::device_count_impl()\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=43537)\u001b[0m [2025-08-12 15:30:23,991 E 43537 43537] logging.cc:440: *** SIGABRT received at time=1754983823 on cpu 3 ***\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=43537)\u001b[0m [2025-08-12 15:30:23,991 E 43537 43537] logging.cc:440: PC: @     0x74520ee9eb2c  (unknown)  pthread_kill\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=43537)\u001b[0m [2025-08-12 15:30:23,991 E 43537 43537] logging.cc:440:     @     0x74520ee45330  (unknown)  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=43537)\u001b[0m [2025-08-12 15:30:23,991 E 43537 43537] logging.cc:440:     @     0x74520ee4527e         32  raise\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=43537)\u001b[0m [2025-08-12 15:30:23,991 E 43537 43537] logging.cc:440:     @     0x74520ee288ff        192  abort\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=43537)\u001b[0m [2025-08-12 15:30:24,026 E 43537 43537] logging.cc:440:     @     0x744fc9488ed9         48  (unknown)\u001b[32m [repeated 27x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=43537)\u001b[0m [2025-08-12 15:30:23,992 E 43537 43537] logging.cc:440:     @     0x74520eeaddae         64  cfree\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=43537)\u001b[0m [2025-08-12 15:30:24,026 E 43537 43537] logging.cc:440:     @     0x744eb0b3a71b         32  cuInit\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=43537)\u001b[0m [2025-08-12 15:30:24,026 E 43537 43537] logging.cc:440:     @     0x744fc944d99a        256  cudaGetDeviceCount\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=43537)\u001b[0m [2025-08-12 15:30:24,026 E 43537 43537] logging.cc:440:     @     0x7452028ade07  (unknown)  c10::cuda::(anonymous namespace)::device_count_impl()\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=43537)\u001b[0m Fatal Python error: Aborted\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=43537)\u001b[0m Stack (most recent call first):\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=43537)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 412 in _lazy_init\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=43537)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1355 in convert\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=43537)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 928 in _apply\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=43537)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1369 in to\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=43537)\u001b[0m   File \"/tmp/ipykernel_39776/2937135518.py\", line 26 in client_fn\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=43537)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py\", line 96 in handle_legacy_message_from_msgtype\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=43537)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/client/client_app.py\", line 128 in ffn\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=43537)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/client/client_app.py\", line 144 in __call__\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=43537)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 58 in run\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=43537)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/util/tracing/tracing_helper.py\", line 467 in _resume_span\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=43537)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 691 in actor_method_executor\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=43537)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 879 in main_loop\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=43537)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/workers/default_worker.py\", line 289 in <module>\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=43537)\u001b[0m Extension modules: psutil._psutil_linux, psutil._psutil_posix, msgpack._cmsgpack, google._upb._message, setproctitle, yaml._yaml, charset_normalizer.md, requests.packages.charset_normalizer.md, requests.packages.chardet.md, ray._raylet, numpy._core._multiarray_umath, numpy.linalg._umath_linalg, grpc._cython.cygrpc, _cffi_backend, pyarrow.lib, pyarrow._json, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, PIL._imaging, PIL._imagingft, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.lib, pyarrow._compute, pandas._libs.ops, pandas._libs.hashing, pandas._libs.arrays, pandas._libs.tslib, pandas._libs.sparse, pandas._libs.internals, pandas._libs.indexing, pandas._libs.index, pandas._libs.writers, pandas._libs.join, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.json, pandas._libs.parsers, pandas._libs.testing, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, kiwisolver._cext (total: 95)\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Create a list of ClientConfig objects, one for each client\n",
    "client_configs = [{\"partition_id\": i} for i in range(NUM_PARTITIONS)]\n",
    "\n",
    "# Specify the resources each of your clients need\n",
    "# By default, each client will be allocated 1x CPU and 0x GPUs\n",
    "backend_config = {\"client_resources\": {\"num_cpus\": 1, \"num_gpus\": 0.0},\n",
    "                      \"client_configs\": client_configs  # Pass the client configurations\n",
    "                     }\n",
    "\n",
    "# When running on GPU, assign an entire GPU for each client\n",
    "if DEVICE == \"cuda\":\n",
    "    backend_config = {\"client_resources\": {\"num_cpus\": 1, \"num_gpus\": 1.0},\n",
    "                      \"client_configs\": client_configs  # Pass the client configurations\n",
    "                     }\n",
    "    # Refer to our Flower framework documentation for more details about Flower simulations\n",
    "    # and how to set up the `backend_config`\n",
    "\n",
    "# Run simulation\n",
    "run_simulation(\n",
    "    server_app=server,\n",
    "    client_app=client,\n",
    "    num_supernodes=NUM_PARTITIONS,\n",
    "    backend_config=backend_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f9eff0",
   "metadata": {},
   "source": [
    "### Starting with a customized strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "605b886a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      Starting Flower ServerApp, config: num_rounds=3, no round_timeout\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [INIT]\n",
      "\u001b[92mINFO \u001b[0m:      Using initial global parameters provided by strategy\n",
      "\u001b[92mINFO \u001b[0m:      Starting evaluation of initial global parameters\n",
      "\u001b[92mINFO \u001b[0m:      Evaluation returned no results (`None`)\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 1]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 3 clients (out of 10)\n",
      "\u001b[36m(ClientAppActor pid=49675)\u001b[0m free(): double free detected in tcache 2\n",
      "\u001b[36m(ClientAppActor pid=49675)\u001b[0m *** SIGABRT received at time=1754985234 on cpu 4 ***\n",
      "\u001b[36m(ClientAppActor pid=49674)\u001b[0m PC: @     0x74d8fd89eb2c  (unknown)  pthread_kill\n",
      "\u001b[36m(ClientAppActor pid=49675)\u001b[0m     @     0x7dd982a45330  1200500384  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=49675)\u001b[0m     @     0x7dd982a4527e         32  raise\n",
      "\u001b[36m(ClientAppActor pid=49675)\u001b[0m     @     0x7dd982a288ff        192  abort\n",
      "\u001b[36m(ClientAppActor pid=49675)\u001b[0m     @     0x7dd982a297b6        288  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=49675)\u001b[0m     @     0x7dd982aa8ff5         16  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=49675)\u001b[0m     @     0x7dd982aab55f         80  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=49675)\u001b[0m     @     0x7dd982aaddae         64  cfree\n",
      "\u001b[36m(ClientAppActor pid=49675)\u001b[0m     @     0x7dd61e13a71b         32  cuInit\n",
      "\u001b[36m(ClientAppActor pid=49675)\u001b[0m     @     0x7dd74124d99a        256  cudaGetDeviceCount\n",
      "\u001b[36m(ClientAppActor pid=49675)\u001b[0m     @     0x7dd9280dee07  (unknown)  c10::cuda::(anonymous namespace)::device_count_impl()\n",
      "\u001b[36m(ClientAppActor pid=49675)\u001b[0m [2025-08-12 15:53:54,326 E 49675 49675] logging.cc:440: *** SIGABRT received at time=1754985234 on cpu 4 ***\n",
      "\u001b[36m(ClientAppActor pid=49675)\u001b[0m [2025-08-12 15:53:54,326 E 49675 49675] logging.cc:440: PC: @     0x7dd982a9eb2c  (unknown)  pthread_kill\n",
      "\u001b[36m(ClientAppActor pid=49675)\u001b[0m [2025-08-12 15:53:54,326 E 49675 49675] logging.cc:440:     @     0x7dd982a45330  1200500384  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=49675)\u001b[0m [2025-08-12 15:53:54,326 E 49675 49675] logging.cc:440:     @     0x7dd982a4527e         32  raise\n",
      "\u001b[36m(ClientAppActor pid=49675)\u001b[0m [2025-08-12 15:53:54,326 E 49675 49675] logging.cc:440:     @     0x7dd982a288ff        192  abort\n",
      "\u001b[36m(ClientAppActor pid=49675)\u001b[0m [2025-08-12 15:53:54,326 E 49675 49675] logging.cc:440:     @     0x7dd982a297b6        288  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=49675)\u001b[0m [2025-08-12 15:53:54,327 E 49675 49675] logging.cc:440:     @     0x7dd982aa8ff5         16  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=49675)\u001b[0m [2025-08-12 15:53:54,327 E 49675 49675] logging.cc:440:     @     0x7dd982aab55f         80  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=49675)\u001b[0m [2025-08-12 15:53:54,327 E 49675 49675] logging.cc:440:     @     0x7dd982aaddae         64  cfree\n",
      "\u001b[36m(ClientAppActor pid=49675)\u001b[0m [2025-08-12 15:53:54,332 E 49675 49675] logging.cc:440:     @     0x7dd61e315e45         48  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=49675)\u001b[0m [2025-08-12 15:53:54,338 E 49675 49675] logging.cc:440:     @     0x7dd61e329889         48  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=49675)\u001b[0m [2025-08-12 15:53:54,344 E 49675 49675] logging.cc:440:     @     0x7dd61e153bcd         64  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=49675)\u001b[0m [2025-08-12 15:53:54,350 E 49675 49675] logging.cc:440:     @     0x7dd61e154fc5         80  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=49675)\u001b[0m [2025-08-12 15:53:54,355 E 49675 49675] logging.cc:440:     @     0x7dd61e115264        288  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=49675)\u001b[0m [2025-08-12 15:53:54,361 E 49675 49675] logging.cc:440:     @     0x7dd61e1ae812        208  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=49675)\u001b[0m [2025-08-12 15:53:54,361 E 49675 49675] logging.cc:440:     @     0x7dd61e13a71b         32  cuInit\n",
      "\u001b[36m(ClientAppActor pid=49675)\u001b[0m [2025-08-12 15:53:54,361 E 49675 49675] logging.cc:440:     @     0x7dd741234a76       1088  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=49675)\u001b[0m [2025-08-12 15:53:54,361 E 49675 49675] logging.cc:440:     @     0x7dd741239638        320  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=49675)\u001b[0m [2025-08-12 15:53:54,361 E 49675 49675] logging.cc:440:     @     0x7dd982aa1ed3        112  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=49675)\u001b[0m [2025-08-12 15:53:54,361 E 49675 49675] logging.cc:440:     @     0x7dd741288ed9         48  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=49675)\u001b[0m [2025-08-12 15:53:54,361 E 49675 49675] logging.cc:440:     @     0x7dd74124d99a        256  cudaGetDeviceCount\n",
      "\u001b[36m(ClientAppActor pid=49675)\u001b[0m [2025-08-12 15:53:54,361 E 49675 49675] logging.cc:440:     @     0x7dd9280dee07  (unknown)  c10::cuda::(anonymous namespace)::device_count_impl()\n",
      "\u001b[36m(ClientAppActor pid=49675)\u001b[0m Fatal Python error: Aborted\n",
      "\u001b[36m(ClientAppActor pid=49675)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=49675)\u001b[0m Stack (most recent call first):\n",
      "\u001b[36m(ClientAppActor pid=49675)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 412 in _lazy_init\n",
      "\u001b[36m(ClientAppActor pid=49675)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1355 in convert\n",
      "\u001b[36m(ClientAppActor pid=49675)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 955 in _apply\n",
      "\u001b[36m(ClientAppActor pid=49675)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 928 in _apply\n",
      "\u001b[36m(ClientAppActor pid=49675)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1369 in to\n",
      "\u001b[36m(ClientAppActor pid=49675)\u001b[0m   File \"/tmp/ipykernel_39776/2937135518.py\", line 26 in client_fn\n",
      "\u001b[36m(ClientAppActor pid=49675)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py\", line 96 in handle_legacy_message_from_msgtype\n",
      "\u001b[36m(ClientAppActor pid=49675)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/client/client_app.py\", line 128 in ffn\n",
      "\u001b[36m(ClientAppActor pid=49675)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/client/client_app.py\", line 144 in __call__\n",
      "\u001b[36m(ClientAppActor pid=49675)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 58 in run\n",
      "\u001b[36m(ClientAppActor pid=49675)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/util/tracing/tracing_helper.py\", line 467 in _resume_span\n",
      "\u001b[36m(ClientAppActor pid=49675)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 691 in actor_method_executor\n",
      "\u001b[36m(ClientAppActor pid=49675)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 879 in main_loop\n",
      "\u001b[36m(ClientAppActor pid=49675)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/workers/default_worker.py\", line 289 in <module>\n",
      "\u001b[36m(ClientAppActor pid=49675)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=49675)\u001b[0m Extension modules: psutil._psutil_linux, psutil._psutil_posix, msgpack._cmsgpack, google._upb._message, setproctitle, yaml._yaml, charset_normalizer.md, requests.packages.charset_normalizer.md, requests.packages.chardet.md, ray._raylet, numpy._core._multiarray_umath, numpy.linalg._umath_linalg, grpc._cython.cygrpc, _cffi_backend, pyarrow.lib, pyarrow._json, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, PIL._imaging, PIL._imagingft, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.lib, pyarrow._compute, pandas._libs.ops, pandas._libs.hashing, pandas._libs.arrays, pandas._libs.tslib, pandas._libs.sparse, pandas._libs.internals, pandas._libs.indexing, pandas._libs.index, pandas._libs.writers, pandas._libs.join, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.json, pandas._libs.parsers, pandas._libs.testing, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, kiwisolver._cext (total: 95)\n",
      "\u001b[36m(ClientAppActor pid=49678)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=49678)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=49674)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=49674)\u001b[0m \n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 529b4d2434048c3bc28a469b01000000\n",
      "\tpid: 49675\n",
      "\tnamespace: d8ebe658-9f59-4dda-988c-4e0e41260ab7\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 529b4d2434048c3bc28a469b01000000\n",
      "\tpid: 49675\n",
      "\tnamespace: d8ebe658-9f59-4dda-988c-4e0e41260ab7\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff529b4d2434048c3bc28a469b01000000 Worker ID: a1e18a53cced6d2884973cea5756a86282b4da469981d263fb246659 Node ID: bc0e0f0df68077d479f807c51146128472547020d3edb614a677b305 Worker IP address: 172.30.170.62 Worker port: 35443 Worker PID: 49675 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 1761f6e84e7a052f81a9edab01000000\n",
      "\tpid: 49678\n",
      "\tnamespace: d8ebe658-9f59-4dda-988c-4e0e41260ab7\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 1761f6e84e7a052f81a9edab01000000\n",
      "\tpid: 49678\n",
      "\tnamespace: d8ebe658-9f59-4dda-988c-4e0e41260ab7\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff1761f6e84e7a052f81a9edab01000000 Worker ID: 0aa64fe4b218bd8829d8334af52ef70eb15d399770fdb4652d7c9078 Node ID: bc0e0f0df68077d479f807c51146128472547020d3edb614a677b305 Worker IP address: 172.30.170.62 Worker port: 43411 Worker PID: 49678 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 87b79c62979f92bcebd0fa9701000000\n",
      "\tpid: 49674\n",
      "\tnamespace: d8ebe658-9f59-4dda-988c-4e0e41260ab7\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 87b79c62979f92bcebd0fa9701000000\n",
      "\tpid: 49674\n",
      "\tnamespace: d8ebe658-9f59-4dda-988c-4e0e41260ab7\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 0 results and 3 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 3 clients (out of 10)\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 87b79c62979f92bcebd0fa9701000000\n",
      "\tpid: 49674\n",
      "\tnamespace: d8ebe658-9f59-4dda-988c-4e0e41260ab7\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 1761f6e84e7a052f81a9edab01000000\n",
      "\tpid: 49678\n",
      "\tnamespace: d8ebe658-9f59-4dda-988c-4e0e41260ab7\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 529b4d2434048c3bc28a469b01000000\n",
      "\tpid: 49675\n",
      "\tnamespace: d8ebe658-9f59-4dda-988c-4e0e41260ab7\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 87b79c62979f92bcebd0fa9701000000\n",
      "\tpid: 49674\n",
      "\tnamespace: d8ebe658-9f59-4dda-988c-4e0e41260ab7\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 1761f6e84e7a052f81a9edab01000000\n",
      "\tpid: 49678\n",
      "\tnamespace: d8ebe658-9f59-4dda-988c-4e0e41260ab7\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 529b4d2434048c3bc28a469b01000000\n",
      "\tpid: 49675\n",
      "\tnamespace: d8ebe658-9f59-4dda-988c-4e0e41260ab7\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 0 results and 3 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 2]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 3 clients (out of 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff87b79c62979f92bcebd0fa9701000000 Worker ID: 4e5e80e843c1c17bd72336c1ca3869b2507c8fcd3673cfd33d235647 Node ID: bc0e0f0df68077d479f807c51146128472547020d3edb614a677b305 Worker IP address: 172.30.170.62 Worker port: 34333 Worker PID: 49674 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 529b4d2434048c3bc28a469b01000000\n",
      "\tpid: 49675\n",
      "\tnamespace: d8ebe658-9f59-4dda-988c-4e0e41260ab7\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 529b4d2434048c3bc28a469b01000000\n",
      "\tpid: 49675\n",
      "\tnamespace: d8ebe658-9f59-4dda-988c-4e0e41260ab7\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 1761f6e84e7a052f81a9edab01000000\n",
      "\tpid: 49678\n",
      "\tnamespace: d8ebe658-9f59-4dda-988c-4e0e41260ab7\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 87b79c62979f92bcebd0fa9701000000\n",
      "\tpid: 49674\n",
      "\tnamespace: d8ebe658-9f59-4dda-988c-4e0e41260ab7\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 1761f6e84e7a052f81a9edab01000000\n",
      "\tpid: 49678\n",
      "\tnamespace: d8ebe658-9f59-4dda-988c-4e0e41260ab7\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 87b79c62979f92bcebd0fa9701000000\n",
      "\tpid: 49674\n",
      "\tnamespace: d8ebe658-9f59-4dda-988c-4e0e41260ab7\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 0 results and 3 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 3 clients (out of 10)\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 1761f6e84e7a052f81a9edab01000000\n",
      "\tpid: 49678\n",
      "\tnamespace: d8ebe658-9f59-4dda-988c-4e0e41260ab7\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 529b4d2434048c3bc28a469b01000000\n",
      "\tpid: 49675\n",
      "\tnamespace: d8ebe658-9f59-4dda-988c-4e0e41260ab7\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 1761f6e84e7a052f81a9edab01000000\n",
      "\tpid: 49678\n",
      "\tnamespace: d8ebe658-9f59-4dda-988c-4e0e41260ab7\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 87b79c62979f92bcebd0fa9701000000\n",
      "\tpid: 49674\n",
      "\tnamespace: d8ebe658-9f59-4dda-988c-4e0e41260ab7\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 529b4d2434048c3bc28a469b01000000\n",
      "\tpid: 49675\n",
      "\tnamespace: d8ebe658-9f59-4dda-988c-4e0e41260ab7\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 87b79c62979f92bcebd0fa9701000000\n",
      "\tpid: 49674\n",
      "\tnamespace: d8ebe658-9f59-4dda-988c-4e0e41260ab7\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 0 results and 3 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 3]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 3 clients (out of 10)\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 529b4d2434048c3bc28a469b01000000\n",
      "\tpid: 49675\n",
      "\tnamespace: d8ebe658-9f59-4dda-988c-4e0e41260ab7\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 1761f6e84e7a052f81a9edab01000000\n",
      "\tpid: 49678\n",
      "\tnamespace: d8ebe658-9f59-4dda-988c-4e0e41260ab7\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 87b79c62979f92bcebd0fa9701000000\n",
      "\tpid: 49674\n",
      "\tnamespace: d8ebe658-9f59-4dda-988c-4e0e41260ab7\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 529b4d2434048c3bc28a469b01000000\n",
      "\tpid: 49675\n",
      "\tnamespace: d8ebe658-9f59-4dda-988c-4e0e41260ab7\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 1761f6e84e7a052f81a9edab01000000\n",
      "\tpid: 49678\n",
      "\tnamespace: d8ebe658-9f59-4dda-988c-4e0e41260ab7\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 87b79c62979f92bcebd0fa9701000000\n",
      "\tpid: 49674\n",
      "\tnamespace: d8ebe658-9f59-4dda-988c-4e0e41260ab7\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 0 results and 3 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 3 clients (out of 10)\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 1761f6e84e7a052f81a9edab01000000\n",
      "\tpid: 49678\n",
      "\tnamespace: d8ebe658-9f59-4dda-988c-4e0e41260ab7\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 87b79c62979f92bcebd0fa9701000000\n",
      "\tpid: 49674\n",
      "\tnamespace: d8ebe658-9f59-4dda-988c-4e0e41260ab7\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 529b4d2434048c3bc28a469b01000000\n",
      "\tpid: 49675\n",
      "\tnamespace: d8ebe658-9f59-4dda-988c-4e0e41260ab7\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 1761f6e84e7a052f81a9edab01000000\n",
      "\tpid: 49678\n",
      "\tnamespace: d8ebe658-9f59-4dda-988c-4e0e41260ab7\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 87b79c62979f92bcebd0fa9701000000\n",
      "\tpid: 49674\n",
      "\tnamespace: d8ebe658-9f59-4dda-988c-4e0e41260ab7\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 529b4d2434048c3bc28a469b01000000\n",
      "\tpid: 49675\n",
      "\tnamespace: d8ebe658-9f59-4dda-988c-4e0e41260ab7\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 0 results and 3 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [SUMMARY]\n",
      "\u001b[92mINFO \u001b[0m:      Run finished 3 round(s) in 33.38s\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[36m(ClientAppActor pid=49674)\u001b[0m free(): double free detected in tcache 2\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=49674)\u001b[0m *** SIGABRT received at time=1754985234 on cpu 3 ***\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=49678)\u001b[0m PC: @     0x733f78c9eb2c  (unknown)  pthread_kill\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=49674)\u001b[0m     @     0x74d6c4688ed9         48  (unknown)\u001b[32m [repeated 38x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=49674)\u001b[0m     @     0x74d8fd84527e         32  raise\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=49674)\u001b[0m     @     0x74d8fd8288ff        192  abort\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=49674)\u001b[0m     @     0x74d8fd8addae         64  cfree\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=49674)\u001b[0m     @     0x74d598f3a71b         32  cuInit\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=49674)\u001b[0m     @     0x74d6c464d99a        256  cudaGetDeviceCount\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=49674)\u001b[0m     @     0x74d6da3ade07  (unknown)  c10::cuda::(anonymous namespace)::device_count_impl()\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=49674)\u001b[0m [2025-08-12 15:53:54,326 E 49674 49674] logging.cc:440: *** SIGABRT received at time=1754985234 on cpu 3 ***\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=49674)\u001b[0m [2025-08-12 15:53:54,326 E 49674 49674] logging.cc:440: PC: @     0x74d8fd89eb2c  (unknown)  pthread_kill\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=49674)\u001b[0m [2025-08-12 15:53:54,362 E 49674 49674] logging.cc:440:     @     0x74d6c4688ed9         48  (unknown)\u001b[32m [repeated 28x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=49674)\u001b[0m [2025-08-12 15:53:54,326 E 49674 49674] logging.cc:440:     @     0x74d8fd84527e         32  raise\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=49674)\u001b[0m [2025-08-12 15:53:54,326 E 49674 49674] logging.cc:440:     @     0x74d8fd8288ff        192  abort\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=49674)\u001b[0m [2025-08-12 15:53:54,327 E 49674 49674] logging.cc:440:     @     0x74d8fd8addae         64  cfree\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=49674)\u001b[0m [2025-08-12 15:53:54,362 E 49674 49674] logging.cc:440:     @     0x74d598f3a71b         32  cuInit\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=49674)\u001b[0m [2025-08-12 15:53:54,362 E 49674 49674] logging.cc:440:     @     0x74d6c464d99a        256  cudaGetDeviceCount\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=49674)\u001b[0m [2025-08-12 15:53:54,362 E 49674 49674] logging.cc:440:     @     0x74d6da3ade07  (unknown)  c10::cuda::(anonymous namespace)::device_count_impl()\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=49674)\u001b[0m Fatal Python error: Aborted\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=49674)\u001b[0m Stack (most recent call first):\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=49674)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 412 in _lazy_init\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=49674)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1355 in convert\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=49674)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 928 in _apply\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=49674)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1369 in to\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=49674)\u001b[0m   File \"/tmp/ipykernel_39776/2937135518.py\", line 26 in client_fn\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=49674)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py\", line 96 in handle_legacy_message_from_msgtype\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=49674)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/client/client_app.py\", line 128 in ffn\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=49674)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/client/client_app.py\", line 144 in __call__\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=49674)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 58 in run\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=49674)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/util/tracing/tracing_helper.py\", line 467 in _resume_span\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=49674)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 691 in actor_method_executor\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=49674)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 879 in main_loop\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=49674)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/workers/default_worker.py\", line 289 in <module>\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=49674)\u001b[0m Extension modules: psutil._psutil_linux, psutil._psutil_posix, msgpack._cmsgpack, google._upb._message, setproctitle, yaml._yaml, charset_normalizer.md, requests.packages.charset_normalizer.md, requests.packages.chardet.md, ray._raylet, numpy._core._multiarray_umath, numpy.linalg._umath_linalg, grpc._cython.cygrpc, _cffi_backend, pyarrow.lib, pyarrow._json, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, PIL._imaging, PIL._imagingft, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.lib, pyarrow._compute, pandas._libs.ops, pandas._libs.hashing, pandas._libs.arrays, pandas._libs.tslib, pandas._libs.sparse, pandas._libs.internals, pandas._libs.indexing, pandas._libs.index, pandas._libs.writers, pandas._libs.join, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.json, pandas._libs.parsers, pandas._libs.testing, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, kiwisolver._cext (total: 95)\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def server_fn(context: Context) -> ServerAppComponents:\n",
    "    # Create FedAdagrad strategy\n",
    "    strategy = FedAdagrad(\n",
    "        fraction_fit=0.3,\n",
    "        fraction_evaluate=0.3,\n",
    "        min_fit_clients=3,\n",
    "        min_evaluate_clients=3,\n",
    "        min_available_clients=NUM_PARTITIONS,\n",
    "        initial_parameters=ndarrays_to_parameters(params),\n",
    "    )\n",
    "    # Configure the server for 3 rounds of training\n",
    "    config = ServerConfig(num_rounds=3)\n",
    "    return ServerAppComponents(strategy=strategy, config=config)\n",
    "\n",
    "\n",
    "# Create the ServerApp\n",
    "server = ServerApp(server_fn=server_fn)\n",
    "\n",
    "# Run simulation\n",
    "run_simulation(\n",
    "    server_app=server,\n",
    "    client_app=client,\n",
    "    num_supernodes=NUM_PARTITIONS,\n",
    "    backend_config=backend_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3ebd15",
   "metadata": {},
   "source": [
    "### Server-side parameter evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "002fedaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The `evaluate` function will be called by Flower after every round\n",
    "def evaluate(\n",
    "    server_round: int,\n",
    "    parameters: NDArrays,\n",
    "    config: Dict[str, Scalar],\n",
    ") -> Optional[Tuple[float, Dict[str, Scalar]]]:\n",
    "    net = Net().to(DEVICE)\n",
    "    _, _, testloader = load_datasets(0, NUM_PARTITIONS)\n",
    "    set_parameters(net, parameters)  # Update model with the latest parameters\n",
    "    loss, accuracy = test(net, testloader)\n",
    "    print(f\"Server-side evaluation loss {loss} / accuracy {accuracy}\")\n",
    "    return loss, {\"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f0fc34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def server_fn(context: Context) -> ServerAppComponents:\n",
    "    # Create the FedAvg strategy\n",
    "    strategy = FedAvg(\n",
    "        fraction_fit=0.3,\n",
    "        fraction_evaluate=0.3,\n",
    "        min_fit_clients=3,\n",
    "        min_evaluate_clients=3,\n",
    "        min_available_clients=NUM_PARTITIONS,\n",
    "        initial_parameters=ndarrays_to_parameters(params),\n",
    "        evaluate_fn=evaluate,  # Pass the evaluation function\n",
    "    )\n",
    "    # Configure the server for 3 rounds of training\n",
    "    config = ServerConfig(num_rounds=3)\n",
    "    return ServerAppComponents(strategy=strategy, config=config)\n",
    "\n",
    "\n",
    "# Create the ServerApp\n",
    "server = ServerApp(server_fn=server_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c238d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      Starting Flower ServerApp, config: num_rounds=3, no round_timeout\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [INIT]\n",
      "\u001b[92mINFO \u001b[0m:      Using initial global parameters provided by strategy\n",
      "\u001b[92mINFO \u001b[0m:      Starting evaluation of initial global parameters\n",
      "\u001b[92mINFO \u001b[0m:      initial parameters (loss, other metrics): 0.07214355702400208, {'accuracy': 0.1}\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 1]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 3 clients (out of 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server-side evaluation loss 0.07214355702400208 / accuracy 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=54633)\u001b[0m free(): double free detected in tcache 2\n",
      "\u001b[36m(ClientAppActor pid=54633)\u001b[0m *** SIGABRT received at time=1754986241 on cpu 1 ***\n",
      "\u001b[36m(ClientAppActor pid=54633)\u001b[0m PC: @     0x76681f69eb2c  (unknown)  pthread_kill\n",
      "\u001b[36m(ClientAppActor pid=54633)\u001b[0m     @     0x76681f645330  254495344  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=54633)\u001b[0m     @     0x76681f64527e         32  raise\n",
      "\u001b[36m(ClientAppActor pid=54633)\u001b[0m     @     0x76681f6288ff        192  abort\n",
      "\u001b[36m(ClientAppActor pid=54633)\u001b[0m     @     0x76681f6297b6        288  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=54633)\u001b[0m     @     0x76681f6a8ff5         16  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=54633)\u001b[0m     @     0x76681f6ab55f         80  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=54633)\u001b[0m     @     0x76681f6addae         64  cfree\n",
      "\u001b[36m(ClientAppActor pid=54633)\u001b[0m     @     0x7664b7715e45         48  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=54633)\u001b[0m     @     0x7664b7729889         48  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=54633)\u001b[0m     @     0x7664b7553bcd         64  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=54633)\u001b[0m     @     0x7664b7554fc5         80  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=54633)\u001b[0m     @     0x7664b7515264        288  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=54633)\u001b[0m     @     0x7664b75ae812        208  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=54633)\u001b[0m     @     0x7664b753a71b         32  cuInit\n",
      "\u001b[36m(ClientAppActor pid=54633)\u001b[0m     @     0x7665d4434a76       1088  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=54633)\u001b[0m     @     0x7665d4439638        320  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=54633)\u001b[0m     @     0x76681f6a1ed3        112  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=54633)\u001b[0m     @     0x7665d4488ed9         48  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=54633)\u001b[0m     @     0x7665d444d99a        256  cudaGetDeviceCount\n",
      "\u001b[36m(ClientAppActor pid=54633)\u001b[0m     @     0x7667dc0a6e07  (unknown)  c10::cuda::(anonymous namespace)::device_count_impl()\n",
      "\u001b[36m(ClientAppActor pid=54633)\u001b[0m [2025-08-12 16:10:42,043 E 54633 54633] logging.cc:440: *** SIGABRT received at time=1754986242 on cpu 1 ***\n",
      "\u001b[36m(ClientAppActor pid=54633)\u001b[0m [2025-08-12 16:10:42,043 E 54633 54633] logging.cc:440: PC: @     0x76681f69eb2c  (unknown)  pthread_kill\n",
      "\u001b[36m(ClientAppActor pid=54633)\u001b[0m [2025-08-12 16:10:42,043 E 54633 54633] logging.cc:440:     @     0x76681f645330  254495344  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=54633)\u001b[0m [2025-08-12 16:10:42,043 E 54633 54633] logging.cc:440:     @     0x76681f64527e         32  raise\n",
      "\u001b[36m(ClientAppActor pid=54633)\u001b[0m [2025-08-12 16:10:42,043 E 54633 54633] logging.cc:440:     @     0x76681f6288ff        192  abort\n",
      "\u001b[36m(ClientAppActor pid=54633)\u001b[0m [2025-08-12 16:10:42,043 E 54633 54633] logging.cc:440:     @     0x76681f6297b6        288  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=54633)\u001b[0m [2025-08-12 16:10:42,044 E 54633 54633] logging.cc:440:     @     0x76681f6a8ff5         16  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=54633)\u001b[0m [2025-08-12 16:10:42,044 E 54633 54633] logging.cc:440:     @     0x76681f6ab55f         80  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=54633)\u001b[0m [2025-08-12 16:10:42,044 E 54633 54633] logging.cc:440:     @     0x76681f6addae         64  cfree\n",
      "\u001b[36m(ClientAppActor pid=54633)\u001b[0m [2025-08-12 16:10:42,079 E 54633 54633] logging.cc:440:     @     0x7664b753a71b         32  cuInit\n",
      "\u001b[36m(ClientAppActor pid=54633)\u001b[0m [2025-08-12 16:10:42,079 E 54633 54633] logging.cc:440:     @     0x7665d444d99a        256  cudaGetDeviceCount\n",
      "\u001b[36m(ClientAppActor pid=54633)\u001b[0m [2025-08-12 16:10:42,079 E 54633 54633] logging.cc:440:     @     0x7667dc0a6e07  (unknown)  c10::cuda::(anonymous namespace)::device_count_impl()\n",
      "\u001b[36m(ClientAppActor pid=54633)\u001b[0m Fatal Python error: Aborted\n",
      "\u001b[36m(ClientAppActor pid=54633)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=54633)\u001b[0m Stack (most recent call first):\n",
      "\u001b[36m(ClientAppActor pid=54633)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 412 in _lazy_init\n",
      "\u001b[36m(ClientAppActor pid=54633)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1355 in convert\n",
      "\u001b[36m(ClientAppActor pid=54633)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 955 in _apply\n",
      "\u001b[36m(ClientAppActor pid=54633)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 928 in _apply\n",
      "\u001b[36m(ClientAppActor pid=54633)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1369 in to\n",
      "\u001b[36m(ClientAppActor pid=54633)\u001b[0m   File \"/tmp/ipykernel_39776/2937135518.py\", line 26 in client_fn\n",
      "\u001b[36m(ClientAppActor pid=54633)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py\", line 96 in handle_legacy_message_from_msgtype\n",
      "\u001b[36m(ClientAppActor pid=54633)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/client/client_app.py\", line 128 in ffn\n",
      "\u001b[36m(ClientAppActor pid=54633)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/client/client_app.py\", line 144 in __call__\n",
      "\u001b[36m(ClientAppActor pid=54633)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 58 in run\n",
      "\u001b[36m(ClientAppActor pid=54633)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/util/tracing/tracing_helper.py\", line 467 in _resume_span\n",
      "\u001b[36m(ClientAppActor pid=54633)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 691 in actor_method_executor\n",
      "\u001b[36m(ClientAppActor pid=54633)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 879 in main_loop\n",
      "\u001b[36m(ClientAppActor pid=54633)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/workers/default_worker.py\", line 289 in <module>\n",
      "\u001b[36m(ClientAppActor pid=54633)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=54633)\u001b[0m Extension modules: psutil._psutil_linux, psutil._psutil_posix, msgpack._cmsgpack, google._upb._message, setproctitle, yaml._yaml, charset_normalizer.md, requests.packages.charset_normalizer.md, requests.packages.chardet.md, ray._raylet, numpy._core._multiarray_umath, numpy.linalg._umath_linalg, grpc._cython.cygrpc, _cffi_backend, pyarrow.lib, pyarrow._json, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, PIL._imaging, PIL._imagingft, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.lib, pyarrow._compute, pandas._libs.ops, pandas._libs.hashing, pandas._libs.arrays, pandas._libs.tslib, pandas._libs.sparse, pandas._libs.internals, pandas._libs.indexing, pandas._libs.index, pandas._libs.writers, pandas._libs.join, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.json, pandas._libs.parsers, pandas._libs.testing, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, kiwisolver._cext (total: 95)\n",
      "\u001b[36m(ClientAppActor pid=54632)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=54632)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=54634)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=54634)\u001b[0m \n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: e2593d199eb5d8cbb86fb94501000000\n",
      "\tpid: 54632\n",
      "\tnamespace: a2e257e8-60de-4862-9b17-3932c75e731a\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: e2593d199eb5d8cbb86fb94501000000\n",
      "\tpid: 54632\n",
      "\tnamespace: a2e257e8-60de-4862-9b17-3932c75e731a\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffffe2593d199eb5d8cbb86fb94501000000 Worker ID: d6bca6b62941d31e094998cd88ef7060bfe8b670fc4c155492364e44 Node ID: e150773a9c4cdb037a7098b3bf79c1902a90f407f93d62d69f3dae74 Worker IP address: 172.30.170.62 Worker port: 35897 Worker PID: 54632 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 4be77a62368ba46af030511e01000000\n",
      "\tpid: 54633\n",
      "\tnamespace: a2e257e8-60de-4862-9b17-3932c75e731a\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 4be77a62368ba46af030511e01000000\n",
      "\tpid: 54633\n",
      "\tnamespace: a2e257e8-60de-4862-9b17-3932c75e731a\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff4be77a62368ba46af030511e01000000 Worker ID: 184d68fb9742009a006d511dce97179224b571ef336d7ef7f8f52349 Node ID: e150773a9c4cdb037a7098b3bf79c1902a90f407f93d62d69f3dae74 Worker IP address: 172.30.170.62 Worker port: 37633 Worker PID: 54633 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: f19c3045413d3dc5a3a50d4801000000\n",
      "\tpid: 54634\n",
      "\tnamespace: a2e257e8-60de-4862-9b17-3932c75e731a\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: f19c3045413d3dc5a3a50d4801000000\n",
      "\tpid: 54634\n",
      "\tnamespace: a2e257e8-60de-4862-9b17-3932c75e731a\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 0 results and 3 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: fffffffffffffffff19c3045413d3dc5a3a50d4801000000 Worker ID: f1e7e688901e60509787840207914c15ca3642d6171a3b35c11f1847 Node ID: e150773a9c4cdb037a7098b3bf79c1902a90f407f93d62d69f3dae74 Worker IP address: 172.30.170.62 Worker port: 45979 Worker PID: 54634 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      fit progress: (1, 0.07214355702400208, {'accuracy': 0.1}, 32.3603552329987)\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 3 clients (out of 10)\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: f19c3045413d3dc5a3a50d4801000000\n",
      "\tpid: 54634\n",
      "\tnamespace: a2e257e8-60de-4862-9b17-3932c75e731a\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 4be77a62368ba46af030511e01000000\n",
      "\tpid: 54633\n",
      "\tnamespace: a2e257e8-60de-4862-9b17-3932c75e731a\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: e2593d199eb5d8cbb86fb94501000000\n",
      "\tpid: 54632\n",
      "\tnamespace: a2e257e8-60de-4862-9b17-3932c75e731a\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: f19c3045413d3dc5a3a50d4801000000\n",
      "\tpid: 54634\n",
      "\tnamespace: a2e257e8-60de-4862-9b17-3932c75e731a\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 4be77a62368ba46af030511e01000000\n",
      "\tpid: 54633\n",
      "\tnamespace: a2e257e8-60de-4862-9b17-3932c75e731a\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: e2593d199eb5d8cbb86fb94501000000\n",
      "\tpid: 54632\n",
      "\tnamespace: a2e257e8-60de-4862-9b17-3932c75e731a\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server-side evaluation loss 0.07214355702400208 / accuracy 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 0 results and 3 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 2]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 3 clients (out of 10)\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: f19c3045413d3dc5a3a50d4801000000\n",
      "\tpid: 54634\n",
      "\tnamespace: a2e257e8-60de-4862-9b17-3932c75e731a\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: e2593d199eb5d8cbb86fb94501000000\n",
      "\tpid: 54632\n",
      "\tnamespace: a2e257e8-60de-4862-9b17-3932c75e731a\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 4be77a62368ba46af030511e01000000\n",
      "\tpid: 54633\n",
      "\tnamespace: a2e257e8-60de-4862-9b17-3932c75e731a\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: f19c3045413d3dc5a3a50d4801000000\n",
      "\tpid: 54634\n",
      "\tnamespace: a2e257e8-60de-4862-9b17-3932c75e731a\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: e2593d199eb5d8cbb86fb94501000000\n",
      "\tpid: 54632\n",
      "\tnamespace: a2e257e8-60de-4862-9b17-3932c75e731a\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 4be77a62368ba46af030511e01000000\n",
      "\tpid: 54633\n",
      "\tnamespace: a2e257e8-60de-4862-9b17-3932c75e731a\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 0 results and 3 failures\n",
      "\u001b[92mINFO \u001b[0m:      fit progress: (2, 0.07214355702400208, {'accuracy': 0.1}, 42.11574727399966)\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 3 clients (out of 10)\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 4be77a62368ba46af030511e01000000\n",
      "\tpid: 54633\n",
      "\tnamespace: a2e257e8-60de-4862-9b17-3932c75e731a\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: f19c3045413d3dc5a3a50d4801000000\n",
      "\tpid: 54634\n",
      "\tnamespace: a2e257e8-60de-4862-9b17-3932c75e731a\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: e2593d199eb5d8cbb86fb94501000000\n",
      "\tpid: 54632\n",
      "\tnamespace: a2e257e8-60de-4862-9b17-3932c75e731a\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 4be77a62368ba46af030511e01000000\n",
      "\tpid: 54633\n",
      "\tnamespace: a2e257e8-60de-4862-9b17-3932c75e731a\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: f19c3045413d3dc5a3a50d4801000000\n",
      "\tpid: 54634\n",
      "\tnamespace: a2e257e8-60de-4862-9b17-3932c75e731a\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: e2593d199eb5d8cbb86fb94501000000\n",
      "\tpid: 54632\n",
      "\tnamespace: a2e257e8-60de-4862-9b17-3932c75e731a\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server-side evaluation loss 0.07214355702400208 / accuracy 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 0 results and 3 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 3]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 3 clients (out of 10)\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: e2593d199eb5d8cbb86fb94501000000\n",
      "\tpid: 54632\n",
      "\tnamespace: a2e257e8-60de-4862-9b17-3932c75e731a\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 4be77a62368ba46af030511e01000000\n",
      "\tpid: 54633\n",
      "\tnamespace: a2e257e8-60de-4862-9b17-3932c75e731a\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: f19c3045413d3dc5a3a50d4801000000\n",
      "\tpid: 54634\n",
      "\tnamespace: a2e257e8-60de-4862-9b17-3932c75e731a\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: e2593d199eb5d8cbb86fb94501000000\n",
      "\tpid: 54632\n",
      "\tnamespace: a2e257e8-60de-4862-9b17-3932c75e731a\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: f19c3045413d3dc5a3a50d4801000000\n",
      "\tpid: 54634\n",
      "\tnamespace: a2e257e8-60de-4862-9b17-3932c75e731a\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 4be77a62368ba46af030511e01000000\n",
      "\tpid: 54633\n",
      "\tnamespace: a2e257e8-60de-4862-9b17-3932c75e731a\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 0 results and 3 failures\n",
      "\u001b[92mINFO \u001b[0m:      fit progress: (3, 0.07214355702400208, {'accuracy': 0.1}, 50.76509301399892)\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 3 clients (out of 10)\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 4be77a62368ba46af030511e01000000\n",
      "\tpid: 54633\n",
      "\tnamespace: a2e257e8-60de-4862-9b17-3932c75e731a\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: f19c3045413d3dc5a3a50d4801000000\n",
      "\tpid: 54634\n",
      "\tnamespace: a2e257e8-60de-4862-9b17-3932c75e731a\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: e2593d199eb5d8cbb86fb94501000000\n",
      "\tpid: 54632\n",
      "\tnamespace: a2e257e8-60de-4862-9b17-3932c75e731a\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 4be77a62368ba46af030511e01000000\n",
      "\tpid: 54633\n",
      "\tnamespace: a2e257e8-60de-4862-9b17-3932c75e731a\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: e2593d199eb5d8cbb86fb94501000000\n",
      "\tpid: 54632\n",
      "\tnamespace: a2e257e8-60de-4862-9b17-3932c75e731a\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: f19c3045413d3dc5a3a50d4801000000\n",
      "\tpid: 54634\n",
      "\tnamespace: a2e257e8-60de-4862-9b17-3932c75e731a\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server-side evaluation loss 0.07214355702400208 / accuracy 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 0 results and 3 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [SUMMARY]\n",
      "\u001b[92mINFO \u001b[0m:      Run finished 3 round(s) in 50.97s\n",
      "\u001b[92mINFO \u001b[0m:      \tHistory (loss, centralized):\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 0: 0.07214355702400208\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 1: 0.07214355702400208\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 2: 0.07214355702400208\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 3: 0.07214355702400208\n",
      "\u001b[92mINFO \u001b[0m:      \tHistory (metrics, centralized):\n",
      "\u001b[92mINFO \u001b[0m:      \t{'accuracy': [(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1)]}\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[36m(ClientAppActor pid=54634)\u001b[0m free(): double free detected in tcache 2\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=54634)\u001b[0m *** SIGABRT received at time=1754986241 on cpu 2 ***\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=54634)\u001b[0m PC: @     0x70277dc9eb2c  (unknown)  pthread_kill\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=54634)\u001b[0m     @     0x702548a88ed9         48  (unknown)\u001b[32m [repeated 28x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=54634)\u001b[0m     @     0x70277dc4527e         32  raise\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=54634)\u001b[0m     @     0x70277dc288ff        192  abort\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=54634)\u001b[0m     @     0x70277dcaddae         64  cfree\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=54634)\u001b[0m     @     0x702415d3a71b         32  cuInit\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=54634)\u001b[0m     @     0x702548a4d99a        256  cudaGetDeviceCount\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=54634)\u001b[0m     @     0x7027001ade07  (unknown)  c10::cuda::(anonymous namespace)::device_count_impl()\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=54634)\u001b[0m [2025-08-12 16:10:42,043 E 54634 54634] logging.cc:440: *** SIGABRT received at time=1754986242 on cpu 2 ***\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=54634)\u001b[0m [2025-08-12 16:10:42,043 E 54634 54634] logging.cc:440: PC: @     0x70277dc9eb2c  (unknown)  pthread_kill\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=54634)\u001b[0m [2025-08-12 16:10:42,079 E 54634 54634] logging.cc:440:     @     0x702548a88ed9         48  (unknown)\u001b[32m [repeated 38x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=54634)\u001b[0m [2025-08-12 16:10:42,043 E 54634 54634] logging.cc:440:     @     0x70277dc4527e         32  raise\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=54634)\u001b[0m [2025-08-12 16:10:42,043 E 54634 54634] logging.cc:440:     @     0x70277dc288ff        192  abort\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=54634)\u001b[0m [2025-08-12 16:10:42,044 E 54634 54634] logging.cc:440:     @     0x70277dcaddae         64  cfree\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=54634)\u001b[0m [2025-08-12 16:10:42,079 E 54634 54634] logging.cc:440:     @     0x702415d3a71b         32  cuInit\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=54634)\u001b[0m [2025-08-12 16:10:42,079 E 54634 54634] logging.cc:440:     @     0x702548a4d99a        256  cudaGetDeviceCount\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=54634)\u001b[0m [2025-08-12 16:10:42,079 E 54634 54634] logging.cc:440:     @     0x7027001ade07  (unknown)  c10::cuda::(anonymous namespace)::device_count_impl()\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=54634)\u001b[0m Fatal Python error: Aborted\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=54634)\u001b[0m Stack (most recent call first):\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=54634)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 412 in _lazy_init\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=54634)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1355 in convert\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=54634)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 928 in _apply\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=54634)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1369 in to\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=54634)\u001b[0m   File \"/tmp/ipykernel_39776/2937135518.py\", line 26 in client_fn\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=54634)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py\", line 96 in handle_legacy_message_from_msgtype\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=54634)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/client/client_app.py\", line 128 in ffn\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=54634)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/client/client_app.py\", line 144 in __call__\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=54634)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 58 in run\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=54634)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/util/tracing/tracing_helper.py\", line 467 in _resume_span\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=54634)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 691 in actor_method_executor\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=54634)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 879 in main_loop\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=54634)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/workers/default_worker.py\", line 289 in <module>\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=54634)\u001b[0m Extension modules: psutil._psutil_linux, psutil._psutil_posix, msgpack._cmsgpack, google._upb._message, setproctitle, yaml._yaml, charset_normalizer.md, requests.packages.charset_normalizer.md, requests.packages.chardet.md, ray._raylet, numpy._core._multiarray_umath, numpy.linalg._umath_linalg, grpc._cython.cygrpc, _cffi_backend, pyarrow.lib, pyarrow._json, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, PIL._imaging, PIL._imagingft, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.lib, pyarrow._compute, pandas._libs.ops, pandas._libs.hashing, pandas._libs.arrays, pandas._libs.tslib, pandas._libs.sparse, pandas._libs.internals, pandas._libs.indexing, pandas._libs.index, pandas._libs.writers, pandas._libs.join, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.json, pandas._libs.parsers, pandas._libs.testing, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, kiwisolver._cext (total: 95)\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Run simulation\n",
    "run_simulation(\n",
    "    server_app=server,\n",
    "    client_app=client,\n",
    "    num_supernodes=NUM_PARTITIONS,\n",
    "    backend_config=backend_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b56436",
   "metadata": {},
   "source": [
    "### Sending/receiving arbitrary values to/from clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa04e02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of ClientConfig objects, one for each client\n",
    "client_configs = [{\"partition_id\": i} for i in range(NUM_PARTITIONS)]\n",
    "\n",
    "# Specify the resources each of your clients need\n",
    "# By default, each client will be allocated 1x CPU and 0x GPUs\n",
    "backend_config = {\"client_resources\": {\"num_cpus\": 1, \"num_gpus\": 0.0},\n",
    "                      \"client_configs\": client_configs  # Pass the client configurations\n",
    "                     }\n",
    "\n",
    "# When running on GPU, assign an entire GPU for each client\n",
    "if DEVICE == \"cuda\":\n",
    "    backend_config = {\"client_resources\": {\"num_cpus\": 1, \"num_gpus\": 1.0},\n",
    "                      \"client_configs\": client_configs  # Pass the client configurations\n",
    "                     }\n",
    "    # Refer to our Flower framework documentation for more details about Flower simulations\n",
    "    # and how to set up the `backend_config`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a05bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_average(metrics: List[Tuple[int, Metrics]]) -> Metrics:\n",
    "    # Multiply accuracy of each client by number of examples used\n",
    "    accuracies = [num_examples * m[\"accuracy\"] for num_examples, m in metrics]\n",
    "    examples = [num_examples for num_examples, _ in metrics]\n",
    "\n",
    "    # Aggregate and return custom metric (weighted average)\n",
    "    return {\"accuracy\": sum(accuracies) / sum(examples)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dc9a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      Starting Flower ServerApp, config: num_rounds=5, no round_timeout\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [INIT]\n",
      "\u001b[92mINFO \u001b[0m:      Requesting initial parameters from one random client\n",
      "\u001b[36m(ClientAppActor pid=83682)\u001b[0m free(): double free detected in tcache 2\n",
      "\u001b[36m(ClientAppActor pid=83682)\u001b[0m *** SIGABRT received at time=1754577165 on cpu 15 ***\n",
      "\u001b[36m(ClientAppActor pid=83682)\u001b[0m PC: @     0x79325a89eb2c  (unknown)  pthread_kill\n",
      "\u001b[36m(ClientAppActor pid=83682)\u001b[0m     @     0x79325a845330   81972576  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=83682)\u001b[0m     @     0x79325a84527e         32  raise\n",
      "\u001b[36m(ClientAppActor pid=83682)\u001b[0m     @     0x79325a8288ff        192  abort\n",
      "\u001b[36m(ClientAppActor pid=83682)\u001b[0m     @     0x79325a8297b6        288  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=83682)\u001b[0m     @     0x79325a8a8ff5         16  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=83682)\u001b[0m     @     0x79325a8ab55f         80  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=83682)\u001b[0m     @     0x79325a8addae         64  cfree\n",
      "\u001b[36m(ClientAppActor pid=83682)\u001b[0m     @     0x792f20115e45         48  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=83682)\u001b[0m     @     0x792f20129889         48  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=83682)\u001b[0m     @     0x792f1ff53bcd         64  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=83682)\u001b[0m     @     0x792f1ff54fc5         80  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=83682)\u001b[0m     @     0x792f1ff15264        288  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=83682)\u001b[0m     @     0x792f1ffae812        208  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=83682)\u001b[0m     @     0x792f1ff3a71b         32  cuInit\n",
      "\u001b[36m(ClientAppActor pid=83682)\u001b[0m     @     0x793048234a76       1088  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=83682)\u001b[0m     @     0x793048239638        320  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=83682)\u001b[0m     @     0x79325a8a1ed3        112  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=83682)\u001b[0m     @     0x793048288ed9         48  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=83682)\u001b[0m     @     0x79304824d99a        256  cudaGetDeviceCount\n",
      "\u001b[36m(ClientAppActor pid=83682)\u001b[0m     @     0x79322c6a6e07  (unknown)  c10::cuda::(anonymous namespace)::device_count_impl()\n",
      "\u001b[36m(ClientAppActor pid=83682)\u001b[0m [2025-08-07 22:32:45,685 E 83682 83682] logging.cc:440: *** SIGABRT received at time=1754577165 on cpu 15 ***\n",
      "\u001b[36m(ClientAppActor pid=83682)\u001b[0m [2025-08-07 22:32:45,685 E 83682 83682] logging.cc:440: PC: @     0x79325a89eb2c  (unknown)  pthread_kill\n",
      "\u001b[36m(ClientAppActor pid=83682)\u001b[0m [2025-08-07 22:32:45,685 E 83682 83682] logging.cc:440:     @     0x79325a845330   81972576  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=83682)\u001b[0m [2025-08-07 22:32:45,685 E 83682 83682] logging.cc:440:     @     0x79325a84527e         32  raise\n",
      "\u001b[36m(ClientAppActor pid=83682)\u001b[0m [2025-08-07 22:32:45,685 E 83682 83682] logging.cc:440:     @     0x79325a8288ff        192  abort\n",
      "\u001b[36m(ClientAppActor pid=83682)\u001b[0m [2025-08-07 22:32:45,685 E 83682 83682] logging.cc:440:     @     0x79325a8297b6        288  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=83682)\u001b[0m [2025-08-07 22:32:45,686 E 83682 83682] logging.cc:440:     @     0x79325a8a8ff5         16  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=83682)\u001b[0m [2025-08-07 22:32:45,686 E 83682 83682] logging.cc:440:     @     0x79325a8ab55f         80  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=83682)\u001b[0m [2025-08-07 22:32:45,686 E 83682 83682] logging.cc:440:     @     0x79325a8addae         64  cfree\n",
      "\u001b[36m(ClientAppActor pid=83682)\u001b[0m [2025-08-07 22:32:45,690 E 83682 83682] logging.cc:440:     @     0x792f20115e45         48  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=83682)\u001b[0m [2025-08-07 22:32:45,694 E 83682 83682] logging.cc:440:     @     0x792f20129889         48  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=83682)\u001b[0m [2025-08-07 22:32:45,698 E 83682 83682] logging.cc:440:     @     0x792f1ff53bcd         64  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=83682)\u001b[0m [2025-08-07 22:32:45,702 E 83682 83682] logging.cc:440:     @     0x792f1ff54fc5         80  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=83682)\u001b[0m [2025-08-07 22:32:45,706 E 83682 83682] logging.cc:440:     @     0x792f1ff15264        288  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=83682)\u001b[0m [2025-08-07 22:32:45,710 E 83682 83682] logging.cc:440:     @     0x792f1ffae812        208  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=83682)\u001b[0m [2025-08-07 22:32:45,710 E 83682 83682] logging.cc:440:     @     0x792f1ff3a71b         32  cuInit\n",
      "\u001b[36m(ClientAppActor pid=83682)\u001b[0m [2025-08-07 22:32:45,710 E 83682 83682] logging.cc:440:     @     0x793048234a76       1088  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=83682)\u001b[0m [2025-08-07 22:32:45,710 E 83682 83682] logging.cc:440:     @     0x793048239638        320  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=83682)\u001b[0m [2025-08-07 22:32:45,710 E 83682 83682] logging.cc:440:     @     0x79325a8a1ed3        112  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=83682)\u001b[0m [2025-08-07 22:32:45,710 E 83682 83682] logging.cc:440:     @     0x793048288ed9         48  (unknown)\n",
      "\u001b[36m(ClientAppActor pid=83682)\u001b[0m [2025-08-07 22:32:45,710 E 83682 83682] logging.cc:440:     @     0x79304824d99a        256  cudaGetDeviceCount\n",
      "\u001b[36m(ClientAppActor pid=83682)\u001b[0m [2025-08-07 22:32:45,710 E 83682 83682] logging.cc:440:     @     0x79322c6a6e07  (unknown)  c10::cuda::(anonymous namespace)::device_count_impl()\n",
      "\u001b[36m(ClientAppActor pid=83682)\u001b[0m Fatal Python error: Aborted\n",
      "\u001b[36m(ClientAppActor pid=83682)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=83682)\u001b[0m Stack (most recent call first):\n",
      "\u001b[36m(ClientAppActor pid=83682)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 412 in _lazy_init\n",
      "\u001b[36m(ClientAppActor pid=83682)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1355 in convert\n",
      "\u001b[36m(ClientAppActor pid=83682)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 955 in _apply\n",
      "\u001b[36m(ClientAppActor pid=83682)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 928 in _apply\n",
      "\u001b[36m(ClientAppActor pid=83682)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1369 in to\n",
      "\u001b[36m(ClientAppActor pid=83682)\u001b[0m   File \"/tmp/ipykernel_67010/2937135518.py\", line 26 in client_fn\n",
      "\u001b[36m(ClientAppActor pid=83682)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py\", line 96 in handle_legacy_message_from_msgtype\n",
      "\u001b[36m(ClientAppActor pid=83682)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/client/client_app.py\", line 128 in ffn\n",
      "\u001b[36m(ClientAppActor pid=83682)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/client/client_app.py\", line 144 in __call__\n",
      "\u001b[36m(ClientAppActor pid=83682)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 58 in run\n",
      "\u001b[36m(ClientAppActor pid=83682)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/util/tracing/tracing_helper.py\", line 467 in _resume_span\n",
      "\u001b[36m(ClientAppActor pid=83682)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 691 in actor_method_executor\n",
      "\u001b[36m(ClientAppActor pid=83682)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 879 in main_loop\n",
      "\u001b[36m(ClientAppActor pid=83682)\u001b[0m   File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/workers/default_worker.py\", line 289 in <module>\n",
      "\u001b[36m(ClientAppActor pid=83682)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=83682)\u001b[0m Extension modules: psutil._psutil_linux, psutil._psutil_posix, msgpack._cmsgpack, google._upb._message, setproctitle, yaml._yaml, charset_normalizer.md, requests.packages.charset_normalizer.md, requests.packages.chardet.md, ray._raylet, numpy._core._multiarray_umath, numpy.linalg._umath_linalg, grpc._cython.cygrpc, _cffi_backend, pyarrow.lib, pyarrow._json, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, PIL._imaging, PIL._imagingft, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.lib, pyarrow._compute, pandas._libs.ops, pandas._libs.hashing, pandas._libs.arrays, pandas._libs.tslib, pandas._libs.sparse, pandas._libs.internals, pandas._libs.indexing, pandas._libs.index, pandas._libs.writers, pandas._libs.join, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.json, pandas._libs.parsers, pandas._libs.testing, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, kiwisolver._cext (total: 95)\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 6bce59fa6c8f0707772cc5b101000000\n",
      "\tpid: 83682\n",
      "\tnamespace: fab1ca42-3cd2-4b24-8dca-7a04f9cf25bd\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 112, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 480, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 6bce59fa6c8f0707772cc5b101000000\n",
      "\tpid: 83682\n",
      "\tnamespace: fab1ca42-3cd2-4b24-8dca-7a04f9cf25bd\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     ServerApp thread raised an exception: Message contains an Error (reason: <class 'ray.exceptions.ActorDiedError'>:<'The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 6bce59fa6c8f0707772cc5b101000000\n",
      "\tpid: 83682\n",
      "\tnamespace: fab1ca42-3cd2-4b24-8dca-7a04f9cf25bd\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.'>). It originated during client-side execution of a message.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/run_simulation.py\", line 285, in server_th_with_start_checks\n",
      "    updated_context = _run(\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/run_serverapp.py\", line 62, in run\n",
      "    server_app(grid=grid, context=context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/server_app.py\", line 166, in __call__\n",
      "    start_grid(\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/compat/app.py\", line 90, in start_grid\n",
      "    hist = run_fl(\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/server.py\", line 492, in run_fl\n",
      "    hist, elapsed_time = server.fit(\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/server.py\", line 93, in fit\n",
      "    self.parameters = self._get_initial_parameters(server_round=0, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/server.py\", line 284, in _get_initial_parameters\n",
      "    get_parameters_res = random_client.get_parameters(\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/compat/grid_client_proxy.py\", line 63, in get_parameters\n",
      "    in_recorddict = self._send_receive_recorddict(\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/compat/grid_client_proxy.py\", line 131, in _send_receive_recorddict\n",
      "    raise ValueError(\n",
      "ValueError: Message contains an Error (reason: <class 'ray.exceptions.ActorDiedError'>:<'The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 6bce59fa6c8f0707772cc5b101000000\n",
      "\tpid: 83682\n",
      "\tnamespace: fab1ca42-3cd2-4b24-8dca-7a04f9cf25bd\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.'>). It originated during client-side execution of a message.\n",
      "\n",
      "Exception in thread Thread-34 (server_th_with_start_checks):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/threading.py\", line 1009, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 772, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/threading.py\", line 946, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/run_simulation.py\", line 285, in server_th_with_start_checks\n",
      "    updated_context = _run(\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/run_serverapp.py\", line 62, in run\n",
      "    server_app(grid=grid, context=context)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/server_app.py\", line 166, in __call__\n",
      "    start_grid(\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/compat/app.py\", line 90, in start_grid\n",
      "    hist = run_fl(\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/server.py\", line 492, in run_fl\n",
      "    hist, elapsed_time = server.fit(\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/server.py\", line 93, in fit\n",
      "    self.parameters = self._get_initial_parameters(server_round=0, timeout=timeout)\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/server.py\", line 284, in _get_initial_parameters\n",
      "    get_parameters_res = random_client.get_parameters(\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/compat/grid_client_proxy.py\", line 63, in get_parameters\n",
      "    in_recorddict = self._send_receive_recorddict(\n",
      "  File \"/home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/server/compat/grid_client_proxy.py\", line 131, in _send_receive_recorddict\n",
      "    raise ValueError(\n",
      "ValueError: Message contains an Error (reason: <class 'ray.exceptions.ActorDiedError'>:<'The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: 6bce59fa6c8f0707772cc5b101000000\n",
      "\tpid: 83682\n",
      "\tnamespace: fab1ca42-3cd2-4b24-8dca-7a04f9cf25bd\n",
      "\tip: 172.30.170.62\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.'>). It originated during client-side execution of a message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff6bce59fa6c8f0707772cc5b101000000 Worker ID: 0433a45b68838a5d294850dcc1e125136ffc78c257eda703db08b793 Node ID: cbe4cf7afefa30bc958ef3f7f8055907a3dee3559c2030dd5dc7609e Worker IP address: 172.30.170.62 Worker port: 44485 Worker PID: 83682 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Exception in ServerApp thread",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 31\u001b[0m\n\u001b[1;32m     26\u001b[0m server \u001b[38;5;241m=\u001b[39m ServerApp(server_fn\u001b[38;5;241m=\u001b[39mserver_fn)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Run the simulation with the client_configs\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m \u001b[43mrun_simulation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_app\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_app\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_supernodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_CLIENTS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackend_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackend_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/run_simulation.py:228\u001b[0m, in \u001b[0;36mrun_simulation\u001b[0;34m(server_app, client_app, num_supernodes, backend_name, backend_config, enable_tf_gpu_growth, verbose_logging)\u001b[0m\n\u001b[1;32m    218\u001b[0m     warn_deprecated_feature_with_example(\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing `enable_tf_gpu_growth=True` is deprecated.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m         example_message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstead, set the `TF_FORCE_GPU_ALLOW_GROWTH` environment \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mflwr.simulation.run_simulationt(...)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    224\u001b[0m     )\n\u001b[1;32m    226\u001b[0m _check_ray_support(backend_name)\n\u001b[0;32m--> 228\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43m_run_simulation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_supernodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_supernodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_app\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_app\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_app\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_app\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackend_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackend_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackend_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackend_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_tf_gpu_growth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_tf_gpu_growth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_logging\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose_logging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexit_event\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEventType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPYTHON_API_RUN_SIMULATION_LEAVE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/run_simulation.py:527\u001b[0m, in \u001b[0;36m_run_simulation\u001b[0;34m(num_supernodes, exit_event, client_app, server_app, backend_name, backend_config, client_app_attr, server_app_attr, server_app_run_config, app_dir, flwr_dir, run, enable_tf_gpu_growth, verbose_logging, is_app)\u001b[0m\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m asyncio_loop_running:\n\u001b[1;32m    524\u001b[0m         \u001b[38;5;66;03m# Set logger propagation to False to prevent duplicated log output in Colab.\u001b[39;00m\n\u001b[1;32m    525\u001b[0m         logger \u001b[38;5;241m=\u001b[39m set_logger_propagation(logger, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 527\u001b[0m     updated_context \u001b[38;5;241m=\u001b[39m \u001b[43m_main_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m updated_context\n",
      "File \u001b[0;32m~/miniconda3/envs/flwr/lib/python3.10/site-packages/flwr/simulation/run_simulation.py:425\u001b[0m, in \u001b[0;36m_main_loop\u001b[0;34m(num_supernodes, backend_name, backend_config_stream, app_dir, is_app, enable_tf_gpu_growth, run, exit_event, flwr_dir, client_app, client_app_attr, server_app, server_app_attr, server_app_run_config)\u001b[0m\n\u001b[1;32m    423\u001b[0m         serverapp_th\u001b[38;5;241m.\u001b[39mjoin()\n\u001b[1;32m    424\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m server_app_thread_has_exception\u001b[38;5;241m.\u001b[39mis_set():\n\u001b[0;32m--> 425\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mException in ServerApp thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    427\u001b[0m log(DEBUG, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopping Simulation Engine now.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m updated_context\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Exception in ServerApp thread"
     ]
    }
   ],
   "source": [
    "def server_fn(context: Context) -> ServerAppComponents:\n",
    "    \"\"\"Construct components that set the ServerApp behaviour.\n",
    "\n",
    "    You can use settings in `context.run_config` to parameterize the\n",
    "    construction of all elements (e.g the strategy or the number of rounds)\n",
    "    wrapped in the returned ServerAppComponents object.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create FedAvg strategy\n",
    "    strategy = FedAvg(\n",
    "        fraction_fit=1.0,\n",
    "        fraction_evaluate=0.5,\n",
    "        min_fit_clients=10,\n",
    "        min_evaluate_clients=5,\n",
    "        min_available_clients=10,\n",
    "        evaluate_metrics_aggregation_fn=weighted_average,  # <-- pass the metric aggregation function\n",
    "    )\n",
    "\n",
    "    # Configure the server for 5 rounds of training\n",
    "    config = ServerConfig(num_rounds=5)\n",
    "\n",
    "    return ServerAppComponents(strategy=strategy, config=config)\n",
    "\n",
    "\n",
    "# Create a new server instance with the updated FedAvg strategy\n",
    "server = ServerApp(server_fn=server_fn)\n",
    "\n",
    "\n",
    "\n",
    "# Run the simulation with the client_configs\n",
    "run_simulation(\n",
    "    server_app=server,\n",
    "    client_app=client,\n",
    "    num_supernodes=NUM_CLIENTS,\n",
    "    backend_config=backend_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317cd5b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flwr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
